{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Applications\n",
    "\n",
    "By Tabita Catalán, Tomás Banduc, David Ortiz y Francisco Sahli, 2025\n",
    "\n",
    "Interesting paper about [parameter estimation](https://www.sciencedirect.com/science/article/pii/S0010482524007911) in blood flow problems.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Building on our previous work with PINNs for forward problems in both nonlinear and linear models, this activity explores the use of PINNs for inverse problems. Specifically, we will apply a PINN to estimate unknown parameters in a diffusion model, such as the diffusion coefficient, by leveraging noisy observations.\n",
    "\n",
    "## Activity Overview\n",
    "\n",
    "In this activity, we will code a PINN to estimate the diffusion coefficient in the linear 1D heat equation.\n",
    "\n",
    "## Activity Goals\n",
    "\n",
    "By the end of this activity, you will be able to:\n",
    "\n",
    " - use the PINN method to solve inverse problems,\n",
    " - train a PINN to simultaneously solve the diffusion equation and estimate an unknown parameter.\n",
    "\n",
    "## Mathematical Description of the Problem\n",
    "\n",
    "Based on the diffusion model from [here](https://deepxde.readthedocs.io/en/latest/demos/pinn_forward/diffusion.1d.html). In this activity, we will consider the one-dimensional diffusion model, also known as the 1D heat equation, as defined in Activity 2:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{3}\n",
    "    \\text{PDE:} \\quad & \\frac{\\partial u}{\\partial t} &&= \\kappa\\frac{\\partial^2 u}{\\partial x^2} - e^{-t}(\\sin(\\pi x) - \\pi^2\\sin(\\pi x)), \\quad && x \\in [-1, 1], \\quad t \\in [0, 2], \\quad \\kappa\\in\\mathbb{R}\\\\\n",
    "    \\text{Solution:} \\quad & u(t,x) &&= e^{-t} \\sin(\\pi x)\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "where $u(t,x)$ represents the quantity of interest (e.g., temperature or concentration) at position $x \\in [-1,1]$ and time $t \\in [0,2]$, and **$\\kappa = 1$ is the diffusion coefficient we want to estimate**. The main objective is to treat $\\kappa$ as a free parameter during PINN training, allowing the network to simultaneously solve the PDE and infer the true value of $\\kappa$.\n",
    "\n",
    "For this inverse problem, we do not require initial and boundary conditions, but instead rely on additional information in the form of noisy observations $u_{data}(t,x)$, which will be detailed below.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Obtain noisy observations using the analytical function\n",
    "2. Sampling the  to train the PINN.\n",
    "3. train a PINN to simultaneously solve the diffusion equation and estimate an unknown parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial setup\n",
    "\n",
    "We begin by importing some usefull packages, and defining some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Import Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "# Import the time module to time our training process\n",
    "import time\n",
    "# Ignore Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Actualización de los parámetros de Matplotlib\n",
    "gray = '#5c5c5c' #'#5c5c5c' '000'\n",
    "mlp.rcParams.update(\n",
    "    {\n",
    "        \"image.cmap\" : 'viridis', # plasma, inferno, magma, cividis\n",
    "        \"text.color\" : gray,\n",
    "        \"xtick.color\" :gray,\n",
    "        \"ytick.color\" :gray,\n",
    "        \"axes.labelcolor\" : gray,\n",
    "        \"axes.edgecolor\" :gray,\n",
    "        \"axes.spines.right\" : False,\n",
    "        \"axes.spines.top\" : False,\n",
    "        \"axes.formatter.use_mathtext\": True,\n",
    "        \"axes.unicode_minus\": False,\n",
    "        \n",
    "        'font.size' : 16,\n",
    "        'interactive': False,\n",
    "        \"font.family\": 'sans-serif',\n",
    "        \"legend.loc\" : 'best',\n",
    "        'text.usetex': False,\n",
    "        'mathtext.fontset': 'stix',\n",
    "    }\n",
    ")\n",
    "\n",
    "# torch definition of pi number\n",
    "torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
    "\n",
    "# Util function to calculate the relative l2 error\n",
    "def relative_l2_error(u_num, u_ref):\n",
    "    # Calculate the L2 norm of the difference\n",
    "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
    "    \n",
    "    # Calculate the L2 norm of the reference\n",
    "    l2_ref = torch.norm(u_ref, p=2)\n",
    "    \n",
    "    # Calculate L2 relative error\n",
    "    relative_l2 = l2_diff / l2_ref\n",
    "    return relative_l2\n",
    "\n",
    "# Util function to plot the solutions\n",
    "def plot_comparison(u_true, u_pred, loss, k_evol):\n",
    "    \n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    u_pred_np = u_pred.detach().numpy()\n",
    "\n",
    "    # Create a figure with 4 subplots\n",
    "    fig1, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot the true values\n",
    "    im1 = axs[0].imshow(u_true, extent=[-1,1,1,0])\n",
    "    axs[0].set_title('Analytic solution for diffusion')\n",
    "    axs[0].set_xlabel(r'$x$')\n",
    "    axs[0].set_ylabel(r'$t$')\n",
    "    fig1.colorbar(im1, spacing='proportional',\n",
    "                            shrink=0.5, ax=axs[0])\n",
    "\n",
    "    # Plot the predicted values\n",
    "    im2 = axs[1].imshow(u_pred_np, extent=[-1,1,1,0])\n",
    "    axs[1].set_title('PINN solution for diffusion')\n",
    "    axs[1].set_xlabel(r'$x$')\n",
    "    axs[1].set_ylabel(r'$t$')\n",
    "    fig1.colorbar(im2, spacing='proportional',\n",
    "                            shrink=0.5, ax=axs[1])\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot the loss values recorded during training\n",
    "    # Create a figure with 2 subplots\n",
    "    fig2, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    # Plot the difference between the predicted and true values\n",
    "    axs[0].plot(k_evol, label=\"PINN estimate\")\n",
    "    axs[0].hlines(1, 0, len(k_evol), label=\"True value\", color=\"tab:green\")\n",
    "    axs[0].set_title(r\"$\\kappa$ evolution\")\n",
    "    axs[0].set_xlabel(\"Iteration\")\n",
    "    \n",
    "    axs[1].plot(loss)\n",
    "    axs[1].set_xlabel('Iteration')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_yscale('log')\n",
    "    axs[1].set_xscale('log')\n",
    "    axs[1].set_title('Training Progress')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Util function to calculate tensor gradients with autodiff   \n",
    "def grad(outputs, inputs):\n",
    "    \"\"\"Computes the partial derivative of an output with respect \n",
    "    to an input.\n",
    "    Args:\n",
    "        outputs: (N, 1) tensor\n",
    "        inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs, \n",
    "                        grad_outputs=torch.ones_like(outputs), \n",
    "                        create_graph=True,\n",
    "                        retain_graph=True,  \n",
    "                        )[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Noisy observations from analytic solution\n",
    "Once again, we define the analytical solution $u(t,x) = e^{-t}\\sin(\\pi x)$. For this task, the `analytic_diffusion` will be used to generate the observation data and as reference for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples in x and t\n",
    "dom_samples = 100\n",
    "\n",
    "# Function for the diffusion analytical solution\n",
    "def analytic_diffusion(x,t):\n",
    "    u = np.exp(-t)*np.sin(np.pi*x)\n",
    "    return u\n",
    "\n",
    "# spatial domain\n",
    "x = np.linspace(-1, 1, dom_samples)\n",
    "# temporal domain\n",
    "t = np.linspace(0, 2, dom_samples)\n",
    "\n",
    "# Domain mesh\n",
    "X, T = np.meshgrid(x, t)\n",
    "U = analytic_diffusion(X, T)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf = ax.plot_surface(X, T, U, cmap='viridis', edgecolor='k')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('t')\n",
    "ax.set_zlabel('u(t, x)')\n",
    "ax.set_title('3D Analytic Solution for Diffusion')\n",
    "\n",
    "# Añadir la barra de color\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sampling the Domain to Train the PINN\n",
    "To train the PINN, we will sample the domain using the Latin Hypercube Sampling (LHS) strategy. LHS ensures that the samples evenly cover the input space, preventing clustering in small areas and distributing them across the entire domain.\n",
    "\n",
    "We import `qmc.LatinHypercube` from `scipy.stats` and scale the samples to match the boundaries of the domain. Additionally, we convert the temporal domain and observations to `torch.tensors` for compatibility with the PINN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "# LHS sampling strategy\n",
    "sampler = qmc.LatinHypercube(d=2)\n",
    "sample = sampler.random(n=100)\n",
    "\n",
    "# lower and upper boundas of the domain\n",
    "l_bounds = [-1, 0]\n",
    "u_bounds = [ 1, 2]\n",
    "domain_xt = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "# torch tensors\n",
    "x_ten = torch.tensor(domain_xt[:, 0], requires_grad = True).float().reshape(-1,1)\n",
    "t_ten = torch.tensor(domain_xt[:, 1], requires_grad = True).float().reshape(-1,1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(domain_xt[:, 0],domain_xt[:, 1], label = 'PDE collocation points')\n",
    "ax.set_title('Collocation points')\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$t$') \n",
    "ax.legend(loc='lower left')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we evaluate the `analytic_diffusion` function on this collocation points and add some noise to get the observation data $u_{data}(t,x)$, i.e.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate sample points in analytical function\n",
    "x_np = x_ten.detach().numpy()\n",
    "t_np = t_ten.detach().numpy()\n",
    "u_true = analytic_diffusion(x_np,t_np).reshape(1, -1)\n",
    "u_observ = u_true + np.random.normal(0,0.01,len(x_np))\n",
    "# convert observations in Pytorch tensors\n",
    "u_observ_t = torch.tensor(u_observ, requires_grad = True).float().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solving the Inverse problem using PINNs\n",
    "\n",
    "As mentioned above, we will perform the $\\kappa$ parameter estimation at the same time as the PINN is trained. To this end, we train the ANN to directly approximate the solution to the partial differential equation and include $\\kappa$ as a free parameters, i.e.,\n",
    "\n",
    "$$\n",
    "u_{PINN}(t, x; \\Theta, \\kappa) \\approx u(t,x)\n",
    "$$\n",
    "\n",
    "where $\\Theta$ are the free (trainable) parameters of the ANN. Now, we use `PyTorch` and define the neural network and, for this task, we will use the ADAM optimizer.\n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> Parameter k is added to the optimizer, not to the ANN\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# training parameters\n",
    "hidden_layers = [2, 20, 20, 20, 1]\n",
    "learning_rate = 0.001\n",
    "training_iter = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# Define a neural network class with user defined layers and neurons\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, hlayers):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params\n",
    "        \n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 921\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neural network \n",
    "u_pinn = NeuralNetwork(hidden_layers)\n",
    "nparams = sum(p.numel() for p in u_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "\n",
    "# treat k as a learnable parameter\n",
    "kappa = torch.nn.Parameter(torch.ones(1, requires_grad=True)*2)\n",
    "kappas = []\n",
    "\n",
    "# add k to the optimiser\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(list(u_pinn.parameters())+[kappa], lr=0.001, \n",
    "                       betas= (0.9,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-Informed Loss function\n",
    "To train the PINN, we recall the diffusion model and define function $f_{pde}(t, x)$ for the PDE. Also, we replace the analytical solution $u(t,x)$ with the PINN output $u_{pinn}(t,x; \\Theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{pde}(t,x;u_{pinn}):=& \\frac{\\partial u}{\\partial t} - \\kappa\\frac{\\partial^2 y}{\\partial x^2} + e^{-t}(\\sin(\\pi x) - \\pi^2  \\sin(\\pi x)) = 0\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> We do not need the boundary and initial conditions\n",
    "</div>\n",
    "\n",
    "Once again we use the $MSE$ and define the physics-informed loss function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta):&= \\frac{\\lambda_1}{N}\\sum_i\\left(f_{pde}(t_i,x_i;u_{pinn})-0\\right)^2 \\quad \\text{PDE loss}\\\\\n",
    "                   & + \\frac{\\lambda_2}{N}\\sum_i\\left(u_{PINN}(t_i,x_i; \\Theta) - \\theta_{data}(t_i,x_i)\\right)^2 \\quad \\text{DATA loss}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\lambda_{1,2}\\in\\mathbb{R}^+$ are positive (weigth) numbers, and $N$ is the number of samples. \n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> For inverse problem data-driven escheme is used.\n",
    "</div>\n",
    "\n",
    "The training is performed by minimizing the loss function $\\mathcal{L}(\\Theta)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PINN_diffusion_Loss(forward_pass, x_ten, t_ten,\n",
    "             lambda1 = 1, lambda2 = 1):\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    domain = torch.cat([t_ten, x_ten], dim = 1)\n",
    "    u = forward_pass(domain)\n",
    "    u_t = grad(u, t_ten)\n",
    "    u_x = grad(u, x_ten)\n",
    "    u_xx = grad(u_x, x_ten)\n",
    "    \n",
    "    # PDE loss definition\n",
    "    f_pde = u_t - kappa*u_xx + torch.exp(-t_ten)*(torch.sin(np.pi*x_ten) \n",
    "                                        -(torch.pi**2)*torch.sin(np.pi*x_ten))\n",
    "    PDE_loss = lambda1 * MSE_func(f_pde, torch.zeros_like(f_pde)) \n",
    "    \n",
    "    # Data loss\n",
    "    data_loss = lambda2 * MSE_func(u, u_observ_t)\n",
    "    \n",
    "    return PDE_loss + data_loss\n",
    "    \n",
    "# Initialize a list to store the loss values\n",
    "loss_values = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = PINN_diffusion_Loss(u_pinn, x_ten, t_ten)\n",
    "    \n",
    "    # Append the current loss value to the list\n",
    "    loss_values.append(loss.item())\n",
    "    kappas.append(kappa.item())\n",
    "\n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ten = torch.tensor(X).float().reshape(-1, 1)\n",
    "T_ten = torch.tensor(T).float().reshape(-1, 1)\n",
    "domain_ten = torch.cat([T_ten, X_ten], dim = 1)\n",
    "U_pred = u_pinn(domain_ten).reshape(dom_samples,dom_samples)\n",
    "\n",
    "U_true = torch.tensor(U).float()\n",
    "print(f'Relative error: {relative_l2_error(U_pred, U_true)}')\n",
    "\n",
    "plot_comparison(U, U_pred, loss_values, kappas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Increase the initial guess for the parameter. Consider that en many applications we only have a searching interval for some parameters.\n",
    "2. increase and decrease the `lambdas` parameters of the loss function\n",
    "3. increase and reduce the learning rate of the optimizer\n",
    "4. change the architecture of the ANN\n",
    "5. increase the number of training iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
