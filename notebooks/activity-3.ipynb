{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MC6DyCDmZUq"
      },
      "source": [
        "# Aplicaciones a Problemas Inversos\n",
        "\n",
        "**Por:** David Ortiz, Rodrigo Salas\n",
        "\n",
        "**Edición:** David Ortiz, Tabita Catalán, Tomás Banduc\n",
        "\n",
        "Paper interesante sobre [estimación de parámetros](https://arxiv.org/abs/2308.00927) en problemas de flujo sanguíneo.\n",
        "\n",
        "## Introducción\n",
        "\n",
        "Basándonos en nuestro trabajo previo con PINNs para resolver problemas directos en modelos no lineales y lineales, esta actividad explora el uso de PINNs para problemas inversos. Específicamente, aplicaremos una PINN para estimar parámetros desconocidos en un modelo de difusión, como el coeficiente de difusión $\\kappa$, a través de observaciones ruidosas.\n",
        "\n",
        "## Resumen\n",
        "\n",
        "En esta actividad, programaremos una PINN para estimar el coeficiente de difusión del modelo lineal de calor en 1D.\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "Al término de esta actividad, será capaz de:\n",
        "\n",
        " - Usar el método de PINN para resolver problemas inversos.\n",
        " - Entrenar una PINN para resolver simultáneamente la ecuación de difusión y estimar el parámetro desconocido del modelo.\n",
        "\n",
        "## Descripción Matemática del Problema\n",
        "\n",
        "Al igual que en la Actividad 2, consideraremos el modelo de difusión unidimensional:\n",
        "$$\n",
        "\\begin{alignat*}{3}\n",
        "    \\text{EDP:} \\quad & \\frac{\\partial u}{\\partial t} &&= \\kappa\\frac{\\partial^2 u}{\\partial x^2} - e^{-t}(\\sin(\\pi x) - \\pi^2\\sin(\\pi x)), \\quad && x \\in [-1, 1], \\quad t \\in [0, 2], \\quad \\kappa\\in\\mathbb{R}\\\\\n",
        "    \\text{Solución:} \\quad & u(t,x) &&= e^{-t} \\sin(\\pi x)\n",
        "\\end{alignat*}\n",
        "$$\n",
        "\n",
        "donde $u(t,x)$ representa una cantidad física de interés en la posición $x \\in [-1,1]$ y en el tiempo $t \\in [0,2]$. $\\kappa$ es el coeficiente de difusión que buscaremos estimar. En este caso, consideraremos $\\kappa=1$ como la solución que deseamos estimar. El principal objetivo es tratar $\\kappa$ como un parámetro libre durante el entrenamiento, permitiendo a la red simultáneamente resolver la EDP e inferir el valor de dicho coeficiente.\n",
        "\n",
        "Para este problema inverso, no necesitamos condiciones iniciales o de borde, sino que nos basamos de información adicional dada por observaciones con ruido $u_{data}(t,x)$ de la solución analítica.\n",
        "\n",
        "\n",
        "## Flujo de Trabajo\n",
        "\n",
        "1. Obtener observaciones con ruido de la solución analítica del problema de valor inicial de la ecuación del calor.\n",
        "2. Muestrear el dominio para entrenar la PINN.\n",
        "3. Entrenar la PINN para resolver simultáneamente la ecuación de difusión y estimar el parámetro desconocido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx9qvQjdmZUr"
      },
      "source": [
        "### Configuración Inicial\n",
        "\n",
        "Comenzamos importando módulos y definiendo algunas funciones de utilidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gufADy3jmZUr"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "#%matplotlib widget"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy para operaciones numéricas\n",
        "import numpy as np\n",
        "# PyTorch para construir y entrenar redes neuronales\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# Matplotlib para graficar\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mlp\n",
        "# Time para medir tiempo de entrenamiento\n",
        "import time\n",
        "# Warnings para ignorar mensajes de advertencia\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "A3G-YUetrKY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9zNpH05mZUs"
      },
      "outputs": [],
      "source": [
        "# Actualización de los parámetros de Matplotlib\n",
        "gray = '#5c5c5c' #'#5c5c5c' '000'\n",
        "mlp.rcParams.update(\n",
        "    {\n",
        "        \"image.cmap\" : 'viridis', # plasma, inferno, magma, cividis\n",
        "        \"text.color\" : gray,\n",
        "        \"xtick.color\" :gray,\n",
        "        \"ytick.color\" :gray,\n",
        "        \"axes.labelcolor\" : gray,\n",
        "        \"axes.edgecolor\" :gray,\n",
        "        \"axes.spines.right\" : False,\n",
        "        \"axes.spines.top\" : False,\n",
        "        \"axes.formatter.use_mathtext\": True,\n",
        "        \"axes.unicode_minus\": False,\n",
        "\n",
        "        'font.size' : 16,\n",
        "        'interactive': False,\n",
        "        \"font.family\": 'sans-serif',\n",
        "        \"legend.loc\" : 'best',\n",
        "        'text.usetex': False,\n",
        "        'mathtext.fontset': 'stix',\n",
        "    }\n",
        ")\n",
        "\n",
        "# Definir pi en torch\n",
        "torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
        "\n",
        "# Error l2 relativo\n",
        "def relative_l2_error(u_num, u_ref):\n",
        "    # Calcular norma l2 de diferencia\n",
        "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
        "\n",
        "    # Calcular norma l2 de referencia\n",
        "    l2_ref = torch.norm(u_ref, p=2)\n",
        "\n",
        "    # Calcular norma l2 relativa\n",
        "    relative_l2 = l2_diff / l2_ref\n",
        "    return relative_l2\n",
        "\n",
        "# Función para graficar soluciones\n",
        "def plot_comparison(u_true, u_pred, loss, k_evol):\n",
        "\n",
        "    # Convertir tensores de numpy a arreglos, para graficar\n",
        "    u_pred_np = u_pred.detach().numpy()\n",
        "\n",
        "    # Crear figura con 2 subplots\n",
        "    fig1, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Graficar solución analítica\n",
        "    im1 = axs[0].imshow(u_true, extent=[-1,1,1,0])\n",
        "    axs[0].set_title('Analytic solution for diffusion')\n",
        "    axs[0].set_xlabel(r'$x$')\n",
        "    axs[0].set_ylabel(r'$t$')\n",
        "    fig1.colorbar(im1, spacing='proportional',\n",
        "                            shrink=0.5, ax=axs[0])\n",
        "\n",
        "    # Graficar predicción\n",
        "    im2 = axs[1].imshow(u_pred_np, extent=[-1,1,1,0])\n",
        "    axs[1].set_title('PINN solution for diffusion')\n",
        "    axs[1].set_xlabel(r'$x$')\n",
        "    axs[1].set_ylabel(r'$t$')\n",
        "    fig1.colorbar(im2, spacing='proportional',\n",
        "                            shrink=0.5, ax=axs[1])\n",
        "    # Display de gráfico\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Graficar los valores de la pérdida guardados durante el entrenamiento\n",
        "    # Crear figura con 2 subplots\n",
        "    fig2, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    # Graficar evolución de parámetro de difusión kappa\n",
        "    axs[0].plot(k_evol, label=\"PINN estimate\")\n",
        "    axs[0].hlines(1, 0, len(k_evol), label=\"True value\", color=\"tab:green\")\n",
        "    axs[0].set_title(r\"$\\kappa$ evolution\")\n",
        "    axs[0].set_xlabel(\"Iteration\")\n",
        "\n",
        "    axs[1].plot(loss)\n",
        "    axs[1].set_xlabel('Iteration')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_yscale('log')\n",
        "    axs[1].set_xscale('log')\n",
        "    axs[1].set_title('Training Progress')\n",
        "    axs[1].grid(True)\n",
        "\n",
        "    # Display de gráfico\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Función para calcular gradientes con diferenciación automática\n",
        "def grad(outputs, inputs):\n",
        "    \"\"\"Calcula derivadas parciales de un output con respecto a un input.\n",
        "    Args:\n",
        "        outputs: tensor (N, 1)\n",
        "        inputs: tensor (N, D)\n",
        "    \"\"\"\n",
        "    return torch.autograd.grad(outputs, inputs,\n",
        "                        grad_outputs=torch.ones_like(outputs),\n",
        "                        create_graph=True,\n",
        "                        retain_graph=True,\n",
        "                        )[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezTYQGE1mZUs"
      },
      "source": [
        "## 1. Observaciones Ruidosas a Partir de Solución Analítica\n",
        "Una vez más, definimos la solución analítica $u(t,x) = e^{-t}\\sin(\\pi x)$, que será considerada la solución pura del problema para efectos de comparación. Para esta tarea, `analytic_diffusion` será usada para generar las observaciones de dicha función."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MfR-ZqemZUs"
      },
      "outputs": [],
      "source": [
        "# Número de muestras para espacio y tiempo.\n",
        "dom_samples = 100\n",
        "\n",
        "# Función para solución analítica\n",
        "def analytic_diffusion(x,t):\n",
        "    u = np.exp(-t)*np.sin(np.pi*x)\n",
        "    return u\n",
        "\n",
        "# Dominio espacial\n",
        "x = np.linspace(-1, 1, dom_samples)\n",
        "# Dominio temporal\n",
        "t = np.linspace(0, 2, dom_samples)\n",
        "\n",
        "# Mallado\n",
        "X, T = np.meshgrid(x, t)\n",
        "# Evaluar función en mallado\n",
        "U = analytic_diffusion(X, T)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "surf = ax.plot_surface(X, T, U, cmap='viridis', edgecolor='k')\n",
        "\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('t')\n",
        "ax.set_zlabel('u(t, x)')\n",
        "ax.set_title('3D Analytic Solution for Diffusion')\n",
        "\n",
        "# Añadir la barra de color\n",
        "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjQLki8YmZUs"
      },
      "source": [
        "## 2. Muestreo del Dominio para entrenar la PINN\n",
        "Para entrenar la PINN, aplicaremos la estrategia LHS (*Latin Hypercube Sampling*). Hay que recordar que LHS asegura que las muestras cubran uniformemente el espacio de entrada, previniendo la clusterización en pequeñas áreas del dominio.\n",
        "\n",
        "Importamos `qmc.LatinHypercube` de `scipy.stats` y escalamos las muestras para adaptarlas al borde del dominio. Adicionalmente, convertimos el dominio temporal y las observaciones a `torch.tensors` para compatibilizarlas con la PINN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrUou0elmZUt"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import qmc\n",
        "# Muestreo con LHS\n",
        "sampler = qmc.LatinHypercube(d=2)\n",
        "sample = sampler.random(n=100)\n",
        "\n",
        "# Límites del dominio\n",
        "l_bounds = [-1, 0]\n",
        "u_bounds = [ 1, 2]\n",
        "domain_xt = qmc.scale(sample, l_bounds, u_bounds)\n",
        "\n",
        "# Tensores de torch\n",
        "x_ten = torch.tensor(domain_xt[:, 0], requires_grad = True).float().reshape(-1,1)\n",
        "t_ten = torch.tensor(domain_xt[:, 1], requires_grad = True).float().reshape(-1,1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "ax.scatter(domain_xt[:, 0],domain_xt[:, 1], label = 'PDE collocation points')\n",
        "ax.set_title('Collocation points')\n",
        "ax.set_xlabel(r'$x$')\n",
        "ax.set_ylabel(r'$t$')\n",
        "ax.legend(loc='lower left')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBPdBnaImZUt"
      },
      "source": [
        "Además, evaluamos `analytic_diffusion` en los puntos de colocación. La inclusión de ruido se sigue del supuesto que las observaciones vienen dadas de la superposición entre la solución analítica y alguna variable aleatoria (típicamente gaussiana) generada por defecto durante la medición, i.e., $u_{data}(t,x)=u(t,x)+\\varepsilon$, con $\\varepsilon\\sim \\mathcal{N}(0,\\sigma)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WorttQggmZUt"
      },
      "outputs": [],
      "source": [
        "# Evaluar puntos en función analítica\n",
        "x_np = x_ten.detach().numpy()\n",
        "t_np = t_ten.detach().numpy()\n",
        "u_true = analytic_diffusion(x_np,t_np).reshape(1, -1)\n",
        "u_observ = u_true + np.random.normal(0,0.01,len(x_np))\n",
        "# Convertir observaciones a torch\n",
        "u_observ_t = torch.tensor(u_observ, requires_grad = True).float().reshape(-1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XCH8uHUmZUt"
      },
      "source": [
        "## 3. Resolución de Problema Inverso con PINNs\n",
        "\n",
        "Como se mencionó más arriba, realizaremos la estimación del parámetro $\\kappa$ al mismo tiempo que entrenamos la PINN. Para este fin, entrenamos la ANN para aproximar directamente la solución de la EDP e incluir $\\kappa$ como parámetro libre, i.e.,\n",
        "\n",
        "$$\n",
        "u_{pinn}(t, x; \\Theta, \\kappa) \\approx u(t,x)\n",
        "$$\n",
        "\n",
        "donde $\\Theta$ son los parámetros entrenables de la red. Ahora, usamos `PyTorch` para crear la red neuronal y consideramos el optimizador ADAM.\n",
        "\n",
        "<div class=\"alert alert-info\"\n",
        "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
        "  <strong>OBSERVACIÓN!</strong> $\\kappa$ se agrega al optimizador, no a la red.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayL3MHthmZUt"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# hiper-parámetros de la red\n",
        "hidden_layers = [2, 20, 20, 20, 1]\n",
        "learning_rate = 0.001\n",
        "training_iter = 40000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK142NkEmZUt"
      },
      "outputs": [],
      "source": [
        "# Error cuadrático medio (Mean Squared Error - MSE)\n",
        "MSE_func = nn.MSELoss()\n",
        "\n",
        "# Definir clase de red neuronal con capas y neuronas especificadas por usuario\n",
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, hlayers):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(len(hlayers[:-2])):\n",
        "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
        "            layers.append(nn.Tanh())\n",
        "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.init_params\n",
        "\n",
        "    def init_params(self):\n",
        "        \"\"\"Inicialización de parámetros Xavier Glorot\n",
        "        \"\"\"\n",
        "        def init_normal(m):\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight) # Xavier\n",
        "        self.apply(init_normal)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-Q9i4HxmZUt"
      },
      "outputs": [],
      "source": [
        "# Crear instancia de la NN\n",
        "u_pinn = NeuralNetwork(hidden_layers)\n",
        "nparams = sum(p.numel() for p in u_pinn.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable parameters: {nparams}')\n",
        "\n",
        "\n",
        "# Tratar k como parámetro entrenable\n",
        "kappa = torch.nn.Parameter(torch.ones(1, requires_grad=True)*2)\n",
        "kappas = []\n",
        "\n",
        "# Definir optimizador y agregar k\n",
        "optimizer = optim.Adam(list(u_pinn.parameters())+[kappa], lr=0.001,\n",
        "                       betas= (0.9,0.999), eps = 1e-8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw1unOOOmZUu"
      },
      "source": [
        "### Función de Pérdida Informada por Física\n",
        "\n",
        "Para entrenar la PINN, definimos una función $f_{pde}(t, x)$ asociada a la EDP de calor. Adicionalmente, reemplazamos la solución $u(t,x)$ por el output de la PINN $u_{pinn}(t,x; \\Theta, \\kappa)$;\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "f_{pde}(t,x;u_{pinn}):=& \\frac{\\partial u_{pinn}}{\\partial t} - \\kappa\\frac{\\partial^2 u_{pinn}}{\\partial x^2} + e^{-t}(\\sin(\\pi x) - \\pi^2  \\sin(\\pi x)) = 0\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "<div class=\"alert alert-info\"\n",
        "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
        "  <strong>OBSERVACIÓN!</strong> No necesitamos condiciones iniciales o de borde.\n",
        "</div>\n",
        "\n",
        "Una vez más, usamos el $MSE$ y definimos la función de pérdida con información física:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L}(\\theta):&= \\frac{\\lambda_1}{N}\\sum_i\\left(f_{pde}(t_i,x_i;u_{pinn})-0\\right)^2 \\quad \\text{Pérdida EDP}\\\\\n",
        "                   & + \\frac{\\lambda_2}{N}\\sum_i\\left(u_{pinn}(t_i,x_i; \\Theta) - \\theta_{data}(t_i,x_i)\\right)^2 \\quad \\text{Pérdida Datos}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "donde $\\lambda_{1,2}\\in\\mathbb{R}^+$ son valores de ponderación positivos y $N$ es el número de muestras.\n",
        "\n",
        "<div class=\"alert alert-info\"\n",
        "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
        "  <strong>OBSERVACIÓN!</strong> Para el problema inverso, el esquema utilizado es basado en datos.\n",
        "</div>\n",
        "\n",
        "El entrenamiento se realiza minimizando la función de pérdida $\\mathcal{L}(\\Theta)$, i.e.,\n",
        "\n",
        "$$\n",
        "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMnU5iFMmZUu"
      },
      "outputs": [],
      "source": [
        "def PINN_diffusion_Loss(forward_pass, x_ten, t_ten,\n",
        "             lambda1 = 1, lambda2 = 1):\n",
        "\n",
        "    # output de ANN\n",
        "    domain = torch.cat([t_ten, x_ten], dim = 1)\n",
        "    u = forward_pass(domain)\n",
        "\n",
        "    #### EDP ####\n",
        "    #TODO: Calcular las derivadas asociadas a la EDP\n",
        "    u_t = ...\n",
        "    u_xx = ...\n",
        "\n",
        "    #TODO: Calcular pérdida de la EDP\n",
        "    f_pde = ...\n",
        "    PDE_loss = ...\n",
        "\n",
        "    #### DATOS ####\n",
        "    #TODO: Calcular pérdida de los datos\n",
        "    data_loss = lambda2 * MSE_func(u, u_observ_t)\n",
        "\n",
        "    return PDE_loss + data_loss\n",
        "\n",
        "# Inicializar lista para guardar valores de pérdida\n",
        "loss_values = []\n",
        "\n",
        "# Empezar timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Entrenar red neuronal\n",
        "for i in range(training_iter):\n",
        "\n",
        "    optimizer.zero_grad()   # Reinicializar gradientes para iteración de entrenamiento\n",
        "\n",
        "    # ingresar x, predecir con PINN y obtener pérdida\n",
        "    loss = PINN_diffusion_Loss(u_pinn, x_ten, t_ten)\n",
        "\n",
        "    # Agregar actual valor de pérdida a la lista y agregar valor de kappa actual\n",
        "    loss_values.append(loss.item())\n",
        "    kappas.append(kappa.item())\n",
        "\n",
        "    if i % 1000 == 0:  # Imprimir pérdida cada 1000 iteraciones\n",
        "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
        "\n",
        "    loss.backward() # Paso de retropropagación\n",
        "    optimizer.step() # Actualizar pesos de la red con optimizador\n",
        "\n",
        "# Detener timer y obtener tiempo transcurrido\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Training time: {elapsed_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRc2wEt0mZUu"
      },
      "outputs": [],
      "source": [
        "X_ten = torch.tensor(X).float().reshape(-1, 1)\n",
        "T_ten = torch.tensor(T).float().reshape(-1, 1)\n",
        "domain_ten = torch.cat([T_ten, X_ten], dim = 1)\n",
        "U_pred = u_pinn(domain_ten).reshape(dom_samples,dom_samples)\n",
        "\n",
        "U_true = torch.tensor(U).float()\n",
        "print(f'Relative error: {relative_l2_error(U_pred, U_true)}')\n",
        "\n",
        "plot_comparison(U, U_pred, loss_values, kappas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_6L95FLmZUu"
      },
      "source": [
        "**Ejercicios**:\n",
        "1. Disminuya y aumente la adivinación inicial del parámetro $\\kappa$. Considere que en varias aplicaciones solo se conoce un intervalo de búsqueda para los parámetros desconocidos.\n",
        "2. Evalúe cómo varía la solución de la PINN aumentando y disminuyendo los pesos `lambdas`.\n",
        "3. Evalúe cómo varía la solución de la PINN aumentando y disminuyendo la tasa de aprendizaje y el número de iteraciones de entrenamiento.\n",
        "4. Cambie el número de capas ocultas, neuronas y funciones de activación del modelo de NN.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pinns-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}