{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAONFevTsQ2t"
      },
      "source": [
        "# Aplicaciones a Problemas Directos\n",
        "\n",
        "**Por:** David Ortiz, Rodrigo Salas\n",
        "\n",
        "**Edición:** David Ortiz, Tabita Catalán, Tomás Banduc\n",
        "\n",
        "### Introducción\n",
        "Continuando con nuestro trabajo previo con PINNs en modelos no lineales, esta actividad amplía nuestra exploración hacia ecuaciones diferenciales parciales (EDPs) utilizando PINNs. Nos centraremos en la solución de un modelo de difusión lineal, específicamente la ecuación de calor en 1D, para examinar cómo las PINNs pueden aplicarse a problemas lineales más simples y profundizar en nuestra comprensión de su adaptabilidad a diferentes tipos de modelos de EDP.\n",
        "\n",
        "### Resumen de la Actividad\n",
        "\n",
        "En esta actividad, implementaremos una PINN para resolver la ecuación de calor, un modelo de difusión lineal, en una dimensión de espacio. Esto nos permitirá explorar la capacidad de las PINNs para resolver EDPs lineales de manera efectiva.\n",
        "\n",
        "### Objetivos de la Actividad\n",
        "\n",
        "Al final de la actividad, será capaz de:\n",
        "\n",
        "- Definir problemas de EDP y construir soluciones analíticas ajustadas al modelo.\n",
        "- Entrenar una PINN para estimar EDPs lineales con un enfoque libre de datos.\n",
        "\n",
        "### Descripción Matemática del Problema\n",
        "\n",
        "Esta actividad se centra en el modelo de difusión unidimensional, comunmente conocido como modelo de calor, que se rige por la ecuación de calor\n",
        "\n",
        "$$\n",
        "\\begin{alignat*}{3}\n",
        "    \\frac{\\partial u}{\\partial t} &&= \\kappa\\frac{\\partial^2 u}{\\partial x^2} + f(t,x), \\quad && x \\in [-1, 1], \\quad t \\in [0, 2], \\quad \\kappa \\in \\mathbb{R}\n",
        "\\end{alignat*}\n",
        "$$\n",
        "\n",
        "donde $u(t,x)$ representa una cantidad física de interés (temperatura, concentración, voltaje, etc.) en la posición $x \\in [-1,1]$ en el tiempo $t \\in [0,2]$, $\\kappa$ es el coeficiente de difusión y $f(t,x)$ es un término fuente.\n",
        "\n",
        "Para simplificar el problema, construiremos una solución analítica al modelo y supondremos, sin pérdida de generalidad, que $\\kappa = 1$. Proponemos la siguiente función como solución del problema:\n",
        "\n",
        "$$\n",
        "u(t,x) = e^{-t}\\sin(\\pi x)\n",
        "$$\n",
        "\n",
        "Sustituyendo $u$ en la EDP, se deriva el siguiente problema con sus respectivas condiciones iniciales y de borde:\n",
        "\n",
        "$$\n",
        "\\begin{alignat*}{3}\n",
        "    \\text{EDP:} \\quad & \\frac{\\partial u}{\\partial t} &&= \\kappa\\frac{\\partial^2 u}{\\partial x^2} - e^{-t}(\\sin(\\pi x) - \\pi^2\\sin(\\pi x)), \\quad && x \\in [-1, 1], \\quad t \\in [0, 2], \\quad \\kappa \\in \\mathbb{R} \\\\\n",
        "    \\text{CI:} \\quad & u(0,x) &&= \\sin(\\pi x), && x \\in [-1, 1] \\\\\n",
        "    \\text{CB:} \\quad & u(t,-1) &&= u(t,1) = 0, && t \\in [0, 2] \\\\\n",
        "    \\text{Solución:} \\quad & u(t,x) &&= e^{-t} \\sin(\\pi x)\n",
        "\\end{alignat*}\n",
        "$$\n",
        "\n",
        "## Flujo de Trabajo\n",
        "\n",
        "1. Calcular la solución analítica utilizando un mallado uniforme.\n",
        "2. Muestrear el dominio para entrenar la PINN.\n",
        "3. Definir la función de pérdida informada con física y entrenar la PINN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbbWY0_6sQ2u"
      },
      "source": [
        "### Configuración Inicial\n",
        "\n",
        "Comenzamos importando módulos y definiendo algunas funciones de utilidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHGJriHZsQ2u"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "#%matplotlib widget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDti8oW6ALsY"
      },
      "outputs": [],
      "source": [
        "# NumPy para operaciones numéricas\n",
        "import numpy as np\n",
        "# PyTorch para construir y entrenar redes neuronales\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# Matplotlib para graficar\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mlp\n",
        "# Time para medir tiempo de entrenamiento\n",
        "import time\n",
        "# Warnings para ignorar mensajes de advertencia\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "from scipy.stats import qmc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDsTqOmksQ2v"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Actualización de los parámetros de Matplotlib\n",
        "gray = '#5c5c5c' #'#5c5c5c' '000'\n",
        "mlp.rcParams.update(\n",
        "    {\n",
        "        \"image.cmap\" : 'viridis', # plasma, inferno, magma, cividis\n",
        "        \"text.color\" : gray,\n",
        "        \"xtick.color\" :gray,\n",
        "        \"ytick.color\" :gray,\n",
        "        \"axes.labelcolor\" : gray,\n",
        "        \"axes.edgecolor\" :gray,\n",
        "        \"axes.spines.right\" : False,\n",
        "        \"axes.spines.top\" : False,\n",
        "        \"axes.formatter.use_mathtext\": True,\n",
        "        \"axes.unicode_minus\": False,\n",
        "\n",
        "        'font.size' : 16,\n",
        "        'interactive': False,\n",
        "        \"font.family\": 'sans-serif',\n",
        "        \"legend.loc\" : 'best',\n",
        "        'text.usetex': False,\n",
        "        'mathtext.fontset': 'stix',\n",
        "    }\n",
        ")\n",
        "\n",
        "# Definir pi en torch\n",
        "torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
        "\n",
        "# Error l2 relativo\n",
        "def relative_l2_error(u_num, u_ref):\n",
        "    # Calcular norma l2 de diferencia\n",
        "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
        "\n",
        "    # Calcular norma l2 de referencia\n",
        "    l2_ref = torch.norm(u_ref, p=2)\n",
        "\n",
        "    # Calcular norma l2 relativa\n",
        "    relative_l2 = l2_diff / l2_ref\n",
        "    return relative_l2\n",
        "\n",
        "# Función para graficar soluciones\n",
        "def plot_comparison(u_true, u_pred, loss):\n",
        "\n",
        "    # Convertir tensores de numpy a arreglos, para graficar\n",
        "    u_pred_np = u_pred.detach().numpy()\n",
        "\n",
        "    # Crear figura con 2 subplots\n",
        "    fig1, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Graficar solución analítica\n",
        "    im1 = axs[0].imshow(u_true, extent=[-1,1,1,0])\n",
        "    axs[0].set_title('Analytic solution for diffusion')\n",
        "    axs[0].set_xlabel(r'$x$')\n",
        "    axs[0].set_ylabel(r'$t$')\n",
        "    fig1.colorbar(im1, spacing='proportional',\n",
        "                            shrink=0.5, ax=axs[0])\n",
        "\n",
        "    # Graficar predicción\n",
        "    im2 = axs[1].imshow(u_pred_np, extent=[-1,1,1,0])\n",
        "    axs[1].set_title('PINN solution for diffusion')\n",
        "    axs[1].set_xlabel(r'$x$')\n",
        "    axs[1].set_ylabel(r'$t$')\n",
        "    fig1.colorbar(im2, spacing='proportional',\n",
        "                            shrink=0.5, ax=axs[1])\n",
        "    # Display de gráfico\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Graficar los valores de la pérdida guardados durante el entrenamiento\n",
        "    # Crear figura con 2 subplots\n",
        "    fig2, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    # Graficar diferencia entre valores reales y predicción\n",
        "    difference = np.abs(u_true - u_pred_np)\n",
        "    im3 = axs[0].imshow(difference, extent=[-1,1,1,0])\n",
        "    axs[0].set_title(r'$|u(t,x) - u_{pred}(t,x)|$')\n",
        "    axs[0].set_xlabel(r'$x$')\n",
        "    axs[0].set_ylabel(r'$t$')\n",
        "    fig2.colorbar(im3, spacing='proportional',\n",
        "                            shrink=0.5, ax=axs[0])\n",
        "\n",
        "    axs[1].plot(loss)\n",
        "    axs[1].set_xlabel('Iteration')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_yscale('log')\n",
        "    axs[1].set_xscale('log')\n",
        "    axs[1].set_title('Training Progress')\n",
        "    axs[1].grid(True)\n",
        "\n",
        "    # Display del gráfico\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Función para calcular gradientes con diferenciación automática\n",
        "def grad(outputs, inputs):\n",
        "    \"\"\"Calcula derivadas parciales de un output con respecto a un input.\n",
        "    Args:\n",
        "        outputs: tensor (N, 1)\n",
        "        inputs: tensor (N, D)\n",
        "    \"\"\"\n",
        "    return torch.autograd.grad(outputs, inputs,\n",
        "                        grad_outputs=torch.ones_like(outputs),\n",
        "                        create_graph=True,\n",
        "                        retain_graph=True,\n",
        "                        )[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iipr8LyOsQ2w"
      },
      "source": [
        "## 1. Solución Analítica\n",
        "Utilizamos la solución $u(t,x) = e^{-t}\\sin(\\pi x)$ para el problema de difusión y la evaluamos en coordenadas específicas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fkJ9exh3cFg"
      },
      "outputs": [],
      "source": [
        "def animate(x,t,U):\n",
        "\n",
        "  # Primero, generar figura con subplot correspondiente\n",
        "  fig, ax = plt.subplots(figsize = (10,6))\n",
        "  plt.close()\n",
        "\n",
        "  ax.set_title(r\"heat solution $e^t sin(\\pi x)$\")\n",
        "\n",
        "  ax.set_xlim(x.min(), x.max())\n",
        "  ax.set_ylim(np.floor(U.min()), np.ceil(U.max()))\n",
        "  ax.set_xlabel(r\"$x$\")\n",
        "  ax.set_ylabel(r\"$u(x, t)$\")\n",
        "\n",
        "  # Inicializar etiqueta sin texto\n",
        "  time_label = ax.text(1, 1, \"\", color = \"black\", fontsize = 12)\n",
        "  # Inicializar gráfico sin datos\n",
        "  line, = ax.plot([], [], color = \"black\", lw = 2)\n",
        "\n",
        "  # Definir función de inicialización\n",
        "  def init():\n",
        "      line.set_data([], [])\n",
        "      time_label.set_text(\"\")\n",
        "      return (line,)\n",
        "\n",
        "  # Animar función. Esta función se llama secuencialmente con FuncAnimation\n",
        "  def animate(i):\n",
        "      line.set_data(x, U[i])\n",
        "      time_label.set_text(f\"t = {t[i]:.2f}\")\n",
        "      return (line,)\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
        "                              frames=len(t), interval=50, blit=True)\n",
        "\n",
        "  return anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z8ShguVyCTR"
      },
      "outputs": [],
      "source": [
        "# Número de muestras para espacio y tiempo.\n",
        "# Obs: Podrían tomarse distintos valores de muestreo en tiempo y en espacio, pero se considera un solo valor por simplicidad\n",
        "dom_samples = 100\n",
        "\n",
        "# Función para solución analítica\n",
        "def analytic_diffusion(x,t):\n",
        "    u = np.exp(-t)*np.sin(np.pi*x)\n",
        "    return u\n",
        "\n",
        "# Dominio espacial\n",
        "x = np.linspace(-1, 1, dom_samples)\n",
        "# Dominio temporal\n",
        "t = np.linspace(0, 2, dom_samples)\n",
        "\n",
        "# Mallado\n",
        "X, T = np.meshgrid(x, t)\n",
        "# Evaluar función en mallado\n",
        "U = analytic_diffusion(X, T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-oa8_JQ4A8N"
      },
      "outputs": [],
      "source": [
        "# Correr animación y hacer display\n",
        "anim = animate(x,t,U)\n",
        "rc(\"animation\", html=\"jshtml\")\n",
        "anim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQnX18mcsQ2x"
      },
      "source": [
        "## 2. Muestreo del Dominio para Entrenar con PINN\n",
        "Para entrenar la PINN, vamos a muestrear el dominio usando la estrategia estratificada de Muestreo de Hipercubo Latino (*Latin Hypercube Sampling* - LHS). Este método asegura que se haga un muestreo que cubra uniformemente el espacio de entrada y evita la generación de aglomeraciones mediante la partición del dominio y la obtención aleatorizada de un punto por cada componente de la partición.\n",
        "\n",
        "Importamos `qmc.LatinHypercube` desde `scipy.stats` y escalamos las muestras para que coincidan con los bordes del dominio. Adicionalmente, se convierten las observaciones y dominio temporal a `torch.tensors` para compatibilizarlos con el modelo de PINN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POUvhO2SsQ2x"
      },
      "outputs": [],
      "source": [
        "# LHS\n",
        "sampler = qmc.LatinHypercube(d=2)\n",
        "sample = sampler.random(n=100)\n",
        "\n",
        "# Bordes inferior y superior del dominio\n",
        "l_bounds = [-1, 0]\n",
        "u_bounds = [ 1, 2]\n",
        "domain_xt = qmc.scale(sample, l_bounds, u_bounds)\n",
        "\n",
        "# Tensores de torch\n",
        "x_ten = torch.tensor(domain_xt[:, 0], requires_grad = True).float().reshape(-1,1)\n",
        "t_ten = torch.tensor(domain_xt[:, 1], requires_grad = True).float().reshape(-1,1)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "ax.scatter(domain_xt[:, 0],domain_xt[:, 1], label = 'PDE collocation points')\n",
        "ax.scatter(domain_xt[:, 0],np.zeros_like(domain_xt[:, 1]), label = 'IC collocation points')\n",
        "ax.scatter(np.ones_like(domain_xt[:, 0]),domain_xt[:, 1], label = 'BC1 collocation points')\n",
        "ax.scatter(np.ones_like(domain_xt[:, 1])*-1,domain_xt[:, 1], label = 'BC2 collocation points')\n",
        "ax.set_title('Collocation points')\n",
        "ax.set_xlabel(r'$x$')\n",
        "ax.set_ylabel(r'$t$')\n",
        "ax.legend(loc='lower left')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXlfF7hMsQ2y"
      },
      "source": [
        "## 3. Entrenamiento de la PINN\n",
        "\n",
        "Entrenamos una red neuronal artificial para aproximar directamente la solución de la EDP, i.e.,\n",
        "\n",
        "$$\n",
        "u_{pinn}(t, x; \\Theta) \\approx u(t,x)\n",
        "$$\n",
        "\n",
        "donde $\\Theta$ son los parámetros libres (entrenables) de la red. Ahora, usamos `PyTorch` y definimos la red. Para el entrenamiento consideraremos el optimizador ADAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY9qWtpgsQ2y"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Hiper-parámetros para el entrenamiento\n",
        "hidden_layers = [2, 10, 10, 10, 1]\n",
        "learning_rate = 0.001\n",
        "training_iter = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9ZDoeyNsQ2y"
      },
      "outputs": [],
      "source": [
        "# Error cuadrático medio (Mean Squared Error - MSE)\n",
        "MSE_func = nn.MSELoss()\n",
        "\n",
        "# Definir clase de red neuronal con capas y neuronas especificadas por usuario\n",
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, hlayers):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(len(hlayers[:-2])):\n",
        "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
        "            layers.append(nn.Tanh())\n",
        "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        \"\"\"Inicialización de parámetros Xavier Glorot\n",
        "        \"\"\"\n",
        "        def init_normal(m):\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight) # Xavier\n",
        "        self.apply(init_normal)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H3kwxj-sQ2y"
      },
      "outputs": [],
      "source": [
        "# Crear instancia de la NN\n",
        "u_pinn = NeuralNetwork(hidden_layers)\n",
        "nparams = sum(p.numel() for p in u_pinn.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable parameters: {nparams}')\n",
        "\n",
        "# Definir optimizador (Adam) para entrenar la red\n",
        "optimizer = optim.Adam(u_pinn.parameters(), lr=0.001,\n",
        "                       betas= (0.9,0.999), eps = 1e-8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oo97JHisQ2y"
      },
      "source": [
        "### Función de Pérdida Informada con Física\n",
        "\n",
        "Para entrenar la PINN, definimos las funciones $f_{pde}(t, x)$, $g_{ic}(0)$ and $h_{bc}(0)$ asociadas a la EDP, la condición inicial y condición de borde, respectivamente. Además, reemplazamos la solución $u(t,x)$ por el output de la PINN $u_{pinn}(t,x; \\Theta)$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "f_{pde}(t,x;u_{pinn}):=& \\frac{\\partial u_{pinn}}{\\partial t} - \\frac{\\partial^2 u_{pinn}}{\\partial x^2} + e^{-t}(\\sin(\\pi x) - \\pi^2 \\sin(\\pi x)) = 0\\\\\n",
        "g_{ic}(0,x;u_{pinn}):=&u_{pinn}(0,x; \\Theta) = \\sin(\\pi x)\\\\\n",
        "h_{bc1}(t,-1;u_{pinn}):=&u_{pinn}(t,-1; \\Theta) = 0\\\\\n",
        "h_{bc2}(t,1;u_{pinn}):=&u_{pinn}(t,1; \\Theta) = 0\n",
        "\\end{align*}\n",
        "$$\n",
        "Una vez más, consideramos el error cuadrático medio y definimos la función de pérdida con información de la física del modelo:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L}(\\theta):&= \\frac{\\lambda_1}{N}\\sum_i\\left(f_{pde}(t_i, x_i;u_{pinn})-0\\right)^2 \\quad \\text{(Pérdida EDP)}\\\\\n",
        "                   & + \\frac{\\lambda_2}{N} \\sum_i(g_{ic}(0,x_i;u_{pinn})-\\sin(\\pi x_i))^2 \\quad \\text{(Pérdida CI)}\\\\\n",
        "                   & + \\frac{\\lambda_3}{N} \\sum_i(h_{bc1}(t_i,-1;u_{pinn})-0)^2 \\quad \\text{(Pérdida CB1)}\\\\\n",
        "                   & + \\frac{\\lambda_3}{N} \\sum_i(h_{bc2}(t_i,1;u_{pinn})-0)^2 \\quad \\text{(Pérdida CB2)}\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "donde $\\lambda_{1,2,3}\\in\\mathbb{R}^+$ son valores de ponderación positivos y $N$ es el número de muestras.\n",
        "\n",
        "El entrenamiento se realiza minimizando la función de pérdida $\\mathcal{L}(\\Theta)$:\n",
        "\n",
        "$$\n",
        "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
        "$$\n",
        "\n",
        "<div class=\"alert alert-info\"\n",
        "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
        "  <strong>OBSERVACIÓN!</strong> Cuando se incluye la función de datos en la pérdida, estamos empleando un esquema basado en datos (data-driven scheme). En caso contrario, decimos que estamos usando un esquema libre de datos (data-free scheme).\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8b8tK1lsQ2z"
      },
      "outputs": [],
      "source": [
        "# HINT:\n",
        "def PINN_diffusion_Loss(forward_pass, x_ten, t_ten,\n",
        "             lambda1 = 1, lambda2 = 1, lambda3 = 1):\n",
        "\n",
        "    # output de ANN\n",
        "    domain = torch.cat([t_ten, x_ten], dim = 1)\n",
        "    u = forward_pass(domain)\n",
        "\n",
        "    #### EDP ####\n",
        "    #TODO: Calcular la primera derivada en tiempo\n",
        "    u_t = ...\n",
        "\n",
        "    #TODO: Calcular primera y segunda derivada en espacio\n",
        "    u_x = ...\n",
        "    u_xx = ...\n",
        "\n",
        "    #TODO: Calcular pérdida de la EDP\n",
        "\n",
        "    f_pde = ...\n",
        "    PDE_loss = ...\n",
        "\n",
        "    #### CI ####\n",
        "\n",
        "    #TODO: Calcular pérdida de condición inicial\n",
        "    g_ic = ...\n",
        "    IC_loss = ...\n",
        "\n",
        "    #### CB ####\n",
        "\n",
        "    #TODO: Calcular pérdida de borde x = -1 (BC1)\n",
        "\n",
        "    h_bc1 = ...\n",
        "    BC1_loss = ...\n",
        "\n",
        "    #TODO: Calcular pérdida de borde x = 1 (BC2)\n",
        "\n",
        "    bc2 = ...\n",
        "    h_bc2 = ...\n",
        "    BC2_loss = ...\n",
        "\n",
        "    return PDE_loss + IC_loss + BC1_loss + BC2_loss\n",
        "\n",
        "# Inicializar lista para guardar valores de pérdida\n",
        "loss_values = []\n",
        "\n",
        "# Empezar timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Entrenar red neuronal\n",
        "for i in range(training_iter):\n",
        "\n",
        "    optimizer.zero_grad()   # Reinicializar gradientes para iteración de entrenamiento\n",
        "\n",
        "    # ingresar x, predecir con PINN y obtener pérdida\n",
        "    loss = PINN_diffusion_Loss(u_pinn, x_ten, t_ten)\n",
        "\n",
        "    # Agregar actual valor de pérdida a la lista\n",
        "    loss_values.append(loss.item())\n",
        "\n",
        "    if i % 1000 == 0:  # Print cada 1000 iteraciones\n",
        "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
        "\n",
        "    loss.backward() # Paso de retropropagación\n",
        "    optimizer.step() # Actualizar pesos de la red con optimizador\n",
        "\n",
        "# Detener timer y obtener tiempo transcurrido\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Training time: {elapsed_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKUq3c71sQ2z"
      },
      "outputs": [],
      "source": [
        "# Graficación\n",
        "X_ten = torch.tensor(X).float().reshape(-1, 1)\n",
        "T_ten = torch.tensor(T).float().reshape(-1, 1)\n",
        "domain_ten = torch.cat([T_ten, X_ten], dim = 1)\n",
        "U_pred = u_pinn(domain_ten).reshape(dom_samples,dom_samples)\n",
        "\n",
        "U_true = torch.tensor(U).float()\n",
        "print(f'Relative error: {relative_l2_error(U_pred, U_true)}')\n",
        "\n",
        "plot_comparison(U, U_pred, loss_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FYWSwocsQ2z"
      },
      "source": [
        "## **Ejercicios**:\n",
        "1. Agregar caso basado en datos. **Indicación**: Utilice los mismos puntos de colocación de la PINN.\n",
        "2. Evalúe cómo varía la solución de la PINN aumentando y disminuyendo los pesos `lambdas`.\n",
        "3. Evalúe cómo varía la solución de la PINN aumentando y disminuyendo la tasa de aprendizaje y el número de iteraciones de entrenamiento.\n",
        "4. Cambie el número de capas ocultas, neuronas y funciones de activación del modelo de NN.\n",
        "\n",
        "## **Questions**:\n",
        "1. ¿Por qué no es necesario incluir datos de la solución en el interior del dominio para resolver el problema de EDP con PINNs?\n",
        "   <details>\n",
        "   <summary>Answer</summary>\n",
        "    En el modelo de difusión, las leyes físicas son bien comprendidas y el problema asociado es bien puesto. El modelo de calor describe cómo una cantidad de interés cambia con el tiempo y cómo esta dinámica se relaciona con el espacio. Las propiedades de los operadores lineales asociados a la EDP de calor hacen que las soluciones al problema de difusión sean únicas y estables, siendo solamente necesarias las condiciones iniciales y de borde para conocerlas completamente al interior del dominio. Este comportamiento hace que no sea necesario tomar un enfoque basado en datos para conocer la solución del problema de difusión, haciendo que la PINN pueda prescindir de información empírica y solo tenga que minimizar residuos asociados a la formulación matemática de la EDP.\n",
        "   </details>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pinns-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}