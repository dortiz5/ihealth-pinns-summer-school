{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Applications\n",
    "\n",
    "By David Ortiz, Tabita Catalán, Tomás Banduc y Francisco Sahli, 2024\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Continuing from our previous work with PINNs on nonlinear models, this activity extends our exploration to general partial differential equations (PDEs) using PINNs. We will focus on the solution of a linear diffusion model, specifically the 1D heat equation, to examine how PINNs can be applied to simpler linear problems and deepen our understanding of their adaptability across different types of PDEs.\n",
    "\n",
    "### Activity Overview\n",
    "\n",
    "In this activity, we will implement a PINN to solve the 1D heat equation, a linear diffusion model. This will allow us to explore the PINN’s capacity to handle linear PDEs effectively.\n",
    "\n",
    "### Activity Goals\n",
    "\n",
    "By the end of this activity, you will be able to:\n",
    "\n",
    " - define PDE problems and construct custom analytical solutions\n",
    " - train a PINN to accurately solve linear PDEs (a non-data-driven approach)\n",
    "\n",
    "### Mathematical Description of the Problem\n",
    "\n",
    "This activity focuses on the one-dimensional diffusion model, commonly referred to as the 1D heat equation:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{3}\n",
    "    \\frac{\\partial u}{\\partial t} &&= \\kappa\\frac{\\partial^2 u}{\\partial x^2} + f(t,x), \\quad && x \\in [-1, 1], \\quad t \\in [0, 2], \\quad \\kappa \\in \\mathbb{R}\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "where $u(t,x)$ represents the quantity of interest (such as temperature or concentration) at position $x \\in [-1,1]$ and time $t \\in [0,2]$, $\\kappa$ is the diffusion coefficient, and $f(t,x)$ is a source term.\n",
    "\n",
    "To simplify, we will construct a custom solution for this model. We propose the following analytical function:\n",
    "\n",
    "$$\n",
    "u(t,x) = e^{-t}\\sin(\\pi x)\n",
    "$$\n",
    "\n",
    "By substituting this function into the PDE, we derive the following problem with its corresponding initial and boundary conditions:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{3}\n",
    "    \\text{PDE:} \\quad & \\frac{\\partial u}{\\partial t} &&= \\kappa\\frac{\\partial^2 u}{\\partial x^2} - e^{-t}(\\sin(\\pi x) - \\pi^2\\sin(\\pi x)), \\quad && x \\in [-1, 1], \\quad t \\in [0, 2], \\quad \\kappa \\in \\mathbb{R} \\\\\n",
    "    \\text{IC:} \\quad & u(0,x) &&= \\sin(\\pi x), && x \\in [-1, 1] \\\\\n",
    "    \\text{BC:} \\quad & u(t,-1) &&= u(t,1) = 0, && t \\in [0, 2] \\\\\n",
    "    \\text{Solution:} \\quad & u(t,x) &&= e^{-t} \\sin(\\pi x)\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "For this activity, we will assume $\\kappa = 1$ without loss of generality.\n",
    "\n",
    "## Workflow\n",
    "1. Calculate the analytical solution using a uniform mesh grid.\n",
    "2. Sample the domain to train the PINN.\n",
    "3. Define the **physics-informed** loss function and train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial setup\n",
    "\n",
    "We begin by importing some usefull packages, and defining some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Import Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "# Import the time module to time our training process\n",
    "import time\n",
    "# Ignore Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Actualización de los parámetros de Matplotlib\n",
    "gray = '#5c5c5c' #'#5c5c5c' '000'\n",
    "mlp.rcParams.update(\n",
    "    {\n",
    "        \"image.cmap\" : 'viridis', # plasma, inferno, magma, cividis\n",
    "        \"text.color\" : gray,\n",
    "        \"xtick.color\" :gray,\n",
    "        \"ytick.color\" :gray,\n",
    "        \"axes.labelcolor\" : gray,\n",
    "        \"axes.edgecolor\" :gray,\n",
    "        \"axes.spines.right\" : False,\n",
    "        \"axes.spines.top\" : False,\n",
    "        \"axes.formatter.use_mathtext\": True,\n",
    "        \"axes.unicode_minus\": False,\n",
    "        \n",
    "        'font.size' : 16,\n",
    "        'interactive': False,\n",
    "        \"font.family\": 'sans-serif',\n",
    "        \"legend.loc\" : 'best',\n",
    "        'text.usetex': False,\n",
    "        'mathtext.fontset': 'stix',\n",
    "    }\n",
    ")\n",
    "\n",
    "# torch definition of pi number\n",
    "torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
    "\n",
    "# Util function to calculate the relative l2 error\n",
    "def relative_l2_error(u_num, u_ref):\n",
    "    # Calculate the L2 norm of the difference\n",
    "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
    "    \n",
    "    # Calculate the L2 norm of the reference\n",
    "    l2_ref = torch.norm(u_ref, p=2)\n",
    "    \n",
    "    # Calculate L2 relative error\n",
    "    relative_l2 = l2_diff / l2_ref\n",
    "    return relative_l2\n",
    "\n",
    "# Util function to plot the solutions\n",
    "def plot_comparison(u_true, u_pred, loss):\n",
    "    \n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    u_pred_np = u_pred.detach().numpy()\n",
    "\n",
    "    # Create a figure with 4 subplots\n",
    "    fig1, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot the true values\n",
    "    im1 = axs[0].imshow(u_true, extent=[-1,1,1,0])\n",
    "    axs[0].set_title('Analytic solution for diffusion')\n",
    "    axs[0].set_xlabel(r'$x$')\n",
    "    axs[0].set_ylabel(r'$t$')\n",
    "    fig1.colorbar(im1, spacing='proportional',\n",
    "                            shrink=0.5, ax=axs[0])\n",
    "\n",
    "    # Plot the predicted values\n",
    "    im2 = axs[1].imshow(u_pred_np, extent=[-1,1,1,0])\n",
    "    axs[1].set_title('PINN solution for diffusion')\n",
    "    axs[1].set_xlabel(r'$x$')\n",
    "    axs[1].set_ylabel(r'$t$')\n",
    "    fig1.colorbar(im2, spacing='proportional',\n",
    "                            shrink=0.5, ax=axs[1])\n",
    "        # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot the loss values recorded during training\n",
    "    # Create a figure with  subplots\n",
    "    fig2, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    # Plot the difference between the predicted and true values\n",
    "    difference = np.abs(u_true - u_pred_np)\n",
    "    im3 = axs[0].imshow(difference, extent=[-1,1,1,0])\n",
    "    axs[0].set_title(r'$|u(t,x) - u_{pred}(t,x)|$')\n",
    "    axs[0].set_xlabel(r'$x$')\n",
    "    axs[0].set_ylabel(r'$t$') \n",
    "    fig2.colorbar(im3, spacing='proportional',\n",
    "                            shrink=0.5, ax=axs[0])\n",
    "    \n",
    "    axs[1].plot(loss)\n",
    "    axs[1].set_xlabel('Iteration')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_yscale('log')\n",
    "    axs[1].set_xscale('log')\n",
    "    axs[1].set_title('Training Progress')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Util function to calculate tensor gradients with autodiff\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\"Computes the partial derivative of an output with respect \n",
    "    to an input.\n",
    "    Args:\n",
    "        outputs: (N, 1) tensor\n",
    "        inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs, \n",
    "                        grad_outputs=torch.ones_like(outputs), \n",
    "                        create_graph=True,\n",
    "                        retain_graph=True,  \n",
    "                        )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analytical solution\n",
    "We use the analytical solution $u(t,x) = e^{-t}\\sin(\\pi x)$ for the diffusion problem and evaluate it at specific coordinate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples in x and t\n",
    "dom_samples = 100\n",
    "\n",
    "# Function for the diffusion analytical solution\n",
    "def analytic_diffusion(x,t):\n",
    "    u = np.exp(-t)*np.sin(np.pi*x)\n",
    "    return u\n",
    "\n",
    "# spatial domain\n",
    "x = np.linspace(-1, 1, dom_samples)\n",
    "# temporal domain\n",
    "t = np.linspace(0, 2, dom_samples)\n",
    "\n",
    "# Domain mesh\n",
    "X, T = np.meshgrid(x, t)\n",
    "U = analytic_diffusion(X, T)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf = ax.plot_surface(X, T, U, cmap='viridis', edgecolor='k')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('t')\n",
    "ax.set_zlabel('u(t, x)')\n",
    "ax.set_title('3D Analytic Solution for Diffusion')\n",
    "\n",
    "# Añadir la barra de color\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sampling the Domain to Train the PINN\n",
    "To train the PINN, we will sample the domain using the Latin Hypercube Sampling (LHS) strategy. LHS ensures that the samples evenly cover the input space, preventing clustering in small areas and distributing them across the entire domain.\n",
    "\n",
    "We import `qmc.LatinHypercube` from `scipy.stats` and scale the samples to match the boundaries of the domain. Additionally, we convert the temporal domain and observations to `torch.tensors` for compatibility with the PINN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "# LHS sampling strategy\n",
    "sampler = qmc.LatinHypercube(d=2)\n",
    "sample = sampler.random(n=100)\n",
    "\n",
    "# lower and upper boundas of the domain\n",
    "l_bounds = [-1, 0]\n",
    "u_bounds = [ 1, 2]\n",
    "domain_xt = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "# torch tensors\n",
    "x_ten = torch.tensor(domain_xt[:, 0], requires_grad = True).float().reshape(-1,1)\n",
    "t_ten = torch.tensor(domain_xt[:, 1], requires_grad = True).float().reshape(-1,1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(domain_xt[:, 0],domain_xt[:, 1], label = 'PDE collocation points')\n",
    "ax.scatter(domain_xt[:, 0],np.zeros_like(domain_xt[:, 1]), label = 'IC collocation points')\n",
    "ax.scatter(np.ones_like(domain_xt[:, 0]),domain_xt[:, 1], label = 'BC1 collocation points')\n",
    "ax.scatter(np.ones_like(domain_xt[:, 1])*-1,domain_xt[:, 1], label = 'BC2 collocation points')\n",
    "ax.set_title('Collocation points')\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$t$') \n",
    "ax.legend(loc='lower left')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the PINN\n",
    "\n",
    "We train the artificial neural network to directly approximate the solution to the partial differential equation, i.e.,\n",
    "\n",
    "$$\n",
    "u_{PINN}(t, x; \\Theta) \\approx u(t,x)\n",
    "$$\n",
    "\n",
    "where $\\Theta$ are the free (trainable) parameters of the ANN. Now, we use `PyTorch` and define the neural network and, for this task, we will use the ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# training parameters\n",
    "hidden_layers = [2, 10, 10, 10, 1]\n",
    "learning_rate = 0.001\n",
    "training_iter = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# Define a neural network class with user defined layers and neurons\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, hlayers):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the neural network \n",
    "u_pinn = NeuralNetwork(hidden_layers)\n",
    "nparams = sum(p.numel() for p in u_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(u_pinn.parameters(), lr=0.001, \n",
    "                       betas= (0.9,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-Informed Loss function\n",
    "To train the PINN, we recall the diffusion model and define function $f_{pde}(t, x)$, $g_{ic}(0)$ and $h_{bc}(0)$ for the PDE, the initial condition and the boundary condition. Also, we replace the analytical solution $u(t,x)$ with the PINN output $u_{pinn}(t,x; \\Theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{pde}(t,x;u_{pinn}):=& \\frac{\\partial u}{\\partial t} - \\frac{\\partial^2 y}{\\partial x^2} + e^{-t}(\\sin(\\pi x) - \\pi^2 \\sin(\\pi x)) = 0\\\\\n",
    "g_{ic}(0,x;u_{pinn}):=&u_{pinn}(0,x; \\Theta) = \\sin(\\pi x)\\\\\n",
    "h_{bc1}(t,-1;u_{pinn}):=&u_{pinn}(t,-1; \\Theta) = 0\\\\\n",
    "h_{bc2}(t,1;u_{pinn}):=&u_{pinn}(t,1; \\Theta) = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Once again we use the $MSE$ and define the physics-informed loss function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta):&= \\frac{\\lambda_1}{N}\\sum_i\\left(f_{pde}(t_i, x_i;u_{pinn})-0\\right)^2 \\quad \\text{PDE loss}\\\\\n",
    "                   & + \\frac{\\lambda_2}{N} \\sum_i(g_{ic}(0,x_i;u_{pinn})-\\sin(\\pi x_i))^2 \\quad \\text{IC loss}\\\\\n",
    "                   & + \\frac{\\lambda_3}{N} \\sum_i(h_{bc1}(t_i,-1;u_{pinn})-0)^2 \\quad \\text{BC1 loss}\\\\\n",
    "                   & + \\frac{\\lambda_3}{N} \\sum_i(h_{bc2}(t_i,1;u_{pinn})-0)^2 \\quad \\text{BC2 loss}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\lambda_{1,2,3}\\in\\mathbb{R}^+$ are positive (weigth) numbers, and $N$ is the number of samples. \n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> when we do not include the loss function related to the data, we are employing a data-free scheme; when we include the data, we are employing a data-driven scheme.\n",
    "</div>\n",
    "\n",
    "The training is performed by minimizing the loss function $\\mathcal{L}(\\Theta)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> Autodifferentiation (torch.autograd) is a powerful tool for calculating the gradients of the PINN with respect to its input to evaluate the loss function; for more information, refer to the tutorial.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: \n",
    "def PINN_diffusion_Loss(forward_pass, x_ten, t_ten, \n",
    "             lambda1 = 1, lambda2 = 1, lambda3 = 1):\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    domain = torch.cat([t_ten, x_ten], dim = 1)\n",
    "    u = forward_pass(domain)\n",
    "    #TODO:  Calculate the first and second derivatives\n",
    "    \n",
    "    # PDE loss definition\n",
    "    #TODO: calculate the PDE loss\n",
    "    \n",
    "    # IC loss definition\n",
    "    #TODO: calculate the IC loss\n",
    "\n",
    "    # BC x = -1 definition\n",
    "    #TODO: calculate the BC1 loss\n",
    "    \n",
    "    # BC x = 1 definition\n",
    "    #TODO: calculate the BC2 loss\n",
    "\n",
    "    \n",
    "    return PDE_loss + IC_loss + BC1_loss + BC2_loss\n",
    "    \n",
    "# Initialize a list to store the loss values\n",
    "loss_values = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = PINN_diffusion_Loss(u_pinn, x_ten, t_ten)\n",
    "    \n",
    "    # Append the current loss value to the list\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_ten = torch.tensor(X).float().reshape(-1, 1)\n",
    "T_ten = torch.tensor(T).float().reshape(-1, 1)\n",
    "domain_ten = torch.cat([T_ten, X_ten], dim = 1)\n",
    "U_pred = u_pinn(domain_ten).reshape(dom_samples,dom_samples)\n",
    "\n",
    "U_true = torch.tensor(U).float()\n",
    "print(f'Relative error: {relative_l2_error(U_pred, U_true)}')\n",
    "\n",
    "plot_comparison(U, U_pred, loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise**:\n",
    "1. Add the data driven-case. **HINT**: you must use the same collocation points for the analytic function and the PINN\n",
    "2. Increase and decrease the `lambdas` parameters of the loss function for both the ANN and the PINN.\n",
    "3. Increase and reduce the learning rate of the optimizer, and the number of training iterations.\n",
    "4. Change the number of hidden layers, the number of neurons, the activation functions of the NN model. \n",
    "\n",
    "## **Questions**:\n",
    "1. Why do you think this PDE, despite being more complex in its domain, does not require a data-driven approach?\n",
    "   <details>\n",
    "   <summary>Answer</summary>\n",
    "    In the diffusion problem, the governing physics is well-understood and represented by a linear PDE, which describes how the quantity of interest (e.g., temperature or concentration) changes over time and space. As a linear equation, the diffusion PDE has solutions that are relatively stable and predictable, governed solely by the initial and boundary conditions. This predictable behavior means we don’t need a data-driven approach; instead, the PINN can learn the solution by minimizing the residuals of the PDE itself. The mathematical formulation already encapsulates the dynamics of diffusion, allowing us to solve the equation without relying on empirical data.\n",
    "   </details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
