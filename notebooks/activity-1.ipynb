{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales artificiales (ANN) vs. edes neuronales informadas por la física\n",
    "\n",
    "Por David Ortiz, Tabita Catalán, Tomás Banduc y Francisco Sahli, 2024\n",
    "\n",
    "Accede al trabajo fundacional de las PINNs [aquí](https://www.sciencedirect.com/science/article/pii/S0021999118307125).  \n",
    "\n",
    "### Introducción  \n",
    "\n",
    "Las Redes Neuronales Artificiales (ANNs) son herramientas potentes para resolver tareas complejas basadas en datos. Sin embargo, suelen necesitar grandes volúmenes de datos y pueden carecer de interpretabilidad al aplicarse a sistemas físicos. Las Redes Neuronales Informadas por la Física (PINNs) resuelven este desafío integrando leyes físicas conocidas directamente en el proceso de entrenamiento, lo que las hace especialmente útiles cuando se dispone de ecuaciones gobernantes pero los datos son limitados.  \n",
    "\n",
    "En esta actividad, exploraremos ambos enfoques aplicándolos al modelado del **péndulo oscilante**, un sistema clásico no lineal.  \n",
    "\n",
    "\n",
    "### Resumen de la Actividad  \n",
    "\n",
    "En esta actividad, programaremos una Red Neuronal Artificial (ANN) y una Red Neuronal Informada por la Física (PINN) para resolver el modelo matemático no lineal de un **péndulo oscilante**. Este enfoque resaltará los beneficios de integrar las leyes físicas en la función de pérdida de la red.  \n",
    "\n",
    "\n",
    "### Objetivos de la Actividad  \n",
    "\n",
    "Al finalizar esta actividad, podrás:  \n",
    "\n",
    "- Comprender la necesidad de soluciones numéricas en modelos complejos.  \n",
    "- Reconocer las ventajas de las Redes Neuronales Informadas por la Física (PINNs) sobre las Redes Neuronales Artificiales (ANNs) tradicionales.  \n",
    "- Entrenar PINNs basadas en datos utilizando PyTorch.  \n",
    "- Aplicar PINNs para resolver modelos no lineales.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción Matemática del Péndulo Oscilante  \n",
    "Queremos resolver el problema matemático relacionado con el **péndulo oscilante** [(wiki)](https://en.wikipedia.org/wiki/Pendulum_(mechanics)):  \n",
    "\n",
    "\n",
    "| ![GIF](../data/figures/Oscillating_pendulum.gif) | <img src=\"../data/figures/Pendulum_gravity.svg\" alt=\"Diagrama del proyecto\" width=\"300\"/> |\n",
    "|-------------------------------------------|-------------------------------------------|\n",
    "| Vectores de velocidad y aceleración del péndulo | Diagrama de fuerzas |\n",
    "\n",
    "\n",
    "Suposiciones  \n",
    "- La varilla es rígida y sin masa [(Tarea)](https://en.wikipedia.org/wiki/Elastic_pendulum#:~:text=In%20physics%20and%20mathematics%2C%20in,%2Ddimensional%20spring%2Dmass%20system.).  \n",
    "- El peso es una masa puntual.  \n",
    "- El sistema está en dos dimensiones [(Tarea)](https://www.instagram.com/reel/CffUr64PjCx/?igsh=MWlmM2FscG9oYnp6bw%3D%3D).  \n",
    "- No hay resistencia del aire [(Tarea)](https://www.youtube.com/watch?v=erveOJD_qv4&ab_channel=Lettherebemath).  \n",
    "- El campo gravitacional es uniforme y el soporte no se mueve.  \n",
    "\n",
    "Nos interesa encontrar el ángulo vertical $\\theta(t) \\in [0, 2\\pi)$ tal que:  \n",
    "\n",
    "$$\n",
    "\\frac{d^2\\theta}{dt^2}+\\frac{g}{l}\\sin\\theta=0,\\quad\\theta(0)=\\theta_0,\\quad\\theta'(0)=0,\\quad t\\in\\mathbb{R}, \n",
    "$$  \n",
    "\n",
    "donde $g\\approx9.81m/s^2$, $l$ es la longitud de la varilla y $t$ la variable temporal.  \n",
    "\n",
    "### Revisión sobre ecuaciones diferenciales:  \n",
    "- ¿Por qué esta es una ecuación diferencial no lineal?  \n",
    "- ¿Es una ecuación diferencial ordinaria (ODE) o una ecuación diferencial parcial (PDE)?  \n",
    "- ¿Cuál es el orden y cuál es el grado?  \n",
    "\n",
    "Un método útil es convertir el modelo en un sistema acoplado de EDOs:  \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d\\theta}{dt} &= \\omega, \\quad \\text{velocidad angular}\\\\\n",
    "\\frac{d\\omega}{dt} & = -\\frac{g}{l}\\sin\\theta, \\quad \\text{aceleración angular}\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "### Flujo de Trabajo  \n",
    "1. Calcular la solución numérica del modelo no lineal del péndulo oscilante.  \n",
    "2. Preparar los datos de entrenamiento añadiendo ruido, remuestreando y limitando el tiempo para simular un escenario real.  \n",
    "3. Definir el modelo ANN utilizando la arquitectura de PyTorch y entrenar con los datos preparados. Graficar la solución.  \n",
    "4. Definir el modelo PINN utilizando la arquitectura de PyTorch y entrenar con los datos preparados. Graficar la solución.  \n",
    "5. Comparar.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Configuración Inicial  \n",
    "\n",
    "Comenzamos importando algunos paquetes útiles y definiendo algunas funciones.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Import Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "# Import the time module to time our training process\n",
    "import time\n",
    "# Ignore Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Actualización de los parámetros de Matplotlib\n",
    "gray = '#5c5c5c' #'#5c5c5c' '000'\n",
    "mlp.rcParams.update(\n",
    "    {\n",
    "        \"image.cmap\" : 'viridis', # plasma, inferno, magma, cividis\n",
    "        \"text.color\" : gray,\n",
    "        \"xtick.color\" :gray,\n",
    "        \"ytick.color\" :gray,\n",
    "        \"axes.labelcolor\" : gray,\n",
    "        \"axes.edgecolor\" :gray,\n",
    "        \"axes.spines.right\" : False,\n",
    "        \"axes.spines.top\" : False,\n",
    "        \"axes.formatter.use_mathtext\": True,\n",
    "        \"axes.unicode_minus\": False,\n",
    "        \n",
    "        'font.size' : 15,\n",
    "        'interactive': False,\n",
    "        \"font.family\": 'sans-serif',\n",
    "        \"legend.loc\" : 'best',\n",
    "        'text.usetex': False,\n",
    "        'mathtext.fontset': 'stix',\n",
    "    }\n",
    ")\n",
    "\n",
    "# Util function to calculate the signal-to-noise ratio\n",
    "def calculate_snr(signal, noise):    \n",
    "    # Ensure numpy arrays\n",
    "    signal, noise = np.array(signal), np.array(noise)\n",
    "    \n",
    "    # Calculate the power of the signal and the noise\n",
    "    signal_power = np.mean(signal**2)\n",
    "    noise_power = np.mean(noise**2)\n",
    "    \n",
    "    # Calculate the SNR in decibels (dB)\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "# Util function to calculate the relative l2 error\n",
    "def relative_l2_error(u_num, u_ref):\n",
    "    # Calculate the L2 norm of the difference\n",
    "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
    "    \n",
    "    # Calculate the L2 norm of the reference\n",
    "    l2_ref = torch.norm(u_ref, p=2)\n",
    "    \n",
    "    # Calculate L2 relative error\n",
    "    relative_l2 = l2_diff / l2_ref\n",
    "    return relative_l2\n",
    "\n",
    "# Util function to plot the solutions\n",
    "def plot_comparison(time, theta_true, theta_pred, loss):\n",
    "    \n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    t_np = time.detach().cpu().data.numpy()\n",
    "    theta_pred_np = theta_pred.detach().cpu().data.numpy()\n",
    "\n",
    "    # Create a figure with 2 subplots\n",
    "    _, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot the true and predicted values\n",
    "    axs[0].plot(t_np, theta_true, label = r'$\\theta(t)$ (numerical solution)')\n",
    "    axs[0].plot(t_np, theta_pred_np, label = r'$\\theta_{pred}(t)$ (predicted solution) ')\n",
    "    axs[0].set_title('Angular displacement Numerical Vs. Predicted')\n",
    "    axs[0].set_xlabel(r'Time $(s)$')\n",
    "    axs[0].set_ylabel('Amplitude')\n",
    "    axs[0].set_ylim(-1,1.3)\n",
    "    axs[0].legend(loc='best', frameon=False)\n",
    "\n",
    "\n",
    "    # Plot the difference between the predicted and true values\n",
    "    difference = np.abs(theta_true.reshape(-1,1) - theta_pred_np.reshape(-1,1))\n",
    "    axs[1].plot(t_np, difference)\n",
    "    axs[1].set_title('Absolute Difference')\n",
    "    axs[1].set_xlabel(r'Time $(s)$')\n",
    "    axs[1].set_ylabel(r'$|\\theta(t) - \\theta_{pred}(t)|$')\n",
    "    # Display the plot\n",
    "    plt.legend(loc='best', frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the loss values recorded during training\n",
    "    # Create a figure with 1 subplots\n",
    "    _, axs = plt.subplots(1, 1, figsize=(6, 3))\n",
    "    axs.plot(loss)\n",
    "    axs.set_xlabel('Iteration')\n",
    "    axs.set_ylabel('Loss')\n",
    "    axs.set_yscale('log')\n",
    "    axs.set_xscale('log')\n",
    "    axs.set_title('Training Progress')\n",
    "    axs.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Util function to calculate tensor gradients with autodiff\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\"Computes the partial derivative of an output with respect \n",
    "    to an input.\n",
    "    Args:\n",
    "        outputs: (N, 1) tensor\n",
    "        inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs, \n",
    "                        grad_outputs=torch.ones_like(outputs), \n",
    "                        create_graph=True,\n",
    "                        )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Solución numérica del péndulo oscilante  \n",
    "Para la solución numérica utilizamos el método [Runge-Kutta de cuarto orden](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods) de `scipy`. Comenzamos definiendo los parámetros para este ejemplo, el modelo del péndulo y el dominio:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 9.81  # gravity acceleration (m/s^2)\n",
    "L = 1.0   # Pendulum's rod length (m)\n",
    "theta0 = np.pi / 4  # Initial condition (Position in rads)\n",
    "omega0 = 0.0        # Initial angular speed (rad/s)\n",
    "sample_freq = 100   # sample rate 100Hz\n",
    "\n",
    "# Simulation time\n",
    "t_span = (0, 10)  # from 0 to 10 seconds\n",
    "t_eval = np.linspace(t_span[0], t_span[1], sample_freq*t_span[1])  # Points to be evaluated\n",
    "\n",
    "# We define the system of coupled ODEs\n",
    "def pendulum(t, y):\n",
    "    theta, omega = y\n",
    "    dtheta_dt = omega\n",
    "    domega_dt = -(g / L) * np.sin(theta)\n",
    "    return [dtheta_dt, domega_dt]\n",
    "\n",
    "# Initial conditions\n",
    "y0 = [theta0, omega0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora resolvemos el problema numéricamente utilizando `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# Solve the initial value problem using Runge-Kutta 4th order\n",
    "num_sol = solve_ivp(pendulum, t_span, y0, t_eval=t_eval, method='RK45')\n",
    "\n",
    "# We extract the solutions\n",
    "theta_num = num_sol.y[0]\n",
    "omega_num = num_sol.y[1]\n",
    "\n",
    "# We graph the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t_eval, theta_num, label=r'Angular Displacement $\\theta(t)[rad]$')\n",
    "plt.plot(t_eval, omega_num, label=r'Angular Velocity $\\omega(t)[rad/s]$')\n",
    "plt.xlabel(r'Time $[s]$')\n",
    "plt.ylim(-2.5,3.3)\n",
    "plt.legend(loc='best', frameon=False)\n",
    "plt.title('Nonlinear Pendulum Solution')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparación de los datos de entrenamiento  \n",
    "\n",
    "A continuación, consideramos la solución numérica como los **datos de entrenamiento** que provienen de las mediciones de un sensor. Añadimos ruido gaussiano, remuestreamos y recortamos los datos a $2.5$ s para evaluar el rendimiento de la ANN, simulando un escenario real. Además, calculamos la relación señal-ruido $SNR = 10\\log_{10} \\left(\\frac{P_{signal}}{P_{noise}}\\right)$, donde $P_{signal}$ y $P_{noise}$ son la potencia de la señal y del ruido, respectivamente, para determinar el nivel de distorsión en la señal. Finalmente, llamamos a los datos de entrenamiento con ruido $\\theta_{data}(t)$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gaussian noise\n",
    "std_deviation = 0.05\n",
    "noise = np.random.normal(0,std_deviation,theta_num.shape[0])\n",
    "theta_noisy = theta_num + noise\n",
    "print(f'SNR: {calculate_snr(theta_noisy, noise):.4f} dB')\n",
    "\n",
    "# Resample and cut to 2.5s\n",
    "resample = 5          # resample \n",
    "cut_time = int(2.5*sample_freq)  # 2.5s times 100Hz\n",
    "\n",
    "theta_data = theta_noisy[:cut_time:resample]\n",
    "t_data = t_eval[:cut_time:resample]\n",
    "\n",
    "# We graph the observed data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t_eval, theta_num, label=r'Angular Displacement (model) $\\theta(t)$ ')\n",
    "plt.plot(t_data, theta_data, label=r'Training data (measures) $\\theta_{data}(t)$ ')\n",
    "plt.xlabel(r'Time $[s]$')\n",
    "plt.ylabel(r'Angular displacement $[rad]$')\n",
    "plt.ylim(-1,1.3)\n",
    "plt.legend(loc='best', frameon=False)\n",
    "plt.title('Training data')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Artificial Neural Network\n",
    "\n",
    "We train the artificial neural network to directly approximate the solution to the differential equation, i.e.,\n",
    "\n",
    "$$\n",
    "\\theta_{NN}(t; \\Theta) \\approx \\theta(t)\n",
    "$$\n",
    "\n",
    "where $\\Theta$ are the free (trainable) parameters of the ANN. Now, we use `PyTorch` and define the neural network and, for this task, we will use the ADAM optimizer. Also, we convert the temporal domain and the observations to `torch.tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# training parameters\n",
    "hidden_layers = [1, 20, 20, 20, 1]\n",
    "learning_rate = 0.001\n",
    "training_iter = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# Convert the NumPy arrays to PyTorch tensors and add an extra dimension\n",
    "# test time Numpy array to Pytorch tensor\n",
    "t_test = torch.tensor(t_eval, device=device, requires_grad=True).view(-1,1).float()\n",
    "# train time Numpy array to Pytorch tensor\n",
    "t_data = torch.tensor(t_data, device=device, requires_grad=True).view(-1,1).float()\n",
    "# Numerical theta to test Numpy array to pytorch tensor \n",
    "theta_test = torch.tensor(theta_num, device=device, requires_grad=True).view(-1,1).float()\n",
    "# Numerical theta to train Numpy array to pytorch tensor \n",
    "theta_data = torch.tensor(theta_data, device=device, requires_grad=True).view(-1,1).float()\n",
    "\n",
    "# Define a neural network class with user defined layers and neurons\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, hlayers):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 901\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neural network \n",
    "theta_ann = NeuralNetwork(hidden_layers).to(device)\n",
    "nparams = sum(p.numel() for p in theta_ann.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(theta_ann.parameters(), lr=learning_rate, \n",
    "                       betas= (0.9,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "To train the ANN, it is mandatory to define the loss function. To this end, we consider the noisy data $\\theta_{data}(t)$ and use the mean squared error ($MSE$) over the colocation points (samples over the domain) $\\{t_i\\}_N$, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta) := \\lambda_1 MSE(\\theta_{NN}(t; \\Theta), \\theta_{data}(t)) = \\frac{\\lambda_1}{N}\\sum_i (\\theta_{NN}(t_i; \\Theta) - \\theta_{data}(t_i))^2\n",
    "$$\n",
    "\n",
    "where $\\lambda_1\\in\\mathbb{R}^+$ is a positive (weigth) number, and $N$ is the number of samples. The training is performed by minimizing the loss function $\\mathcal{L}(\\Theta)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
    "$$\n",
    "\n",
    "\n",
    "Now, we use `PyTorch` and define the neural network, the function loss and, for this task, we will use the ADAM optimizer. Also, we convert the temporal domain and the observations to `torch.tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetworkLoss(forward_pass, t, theta_data, lambda1 = 1):\n",
    "    \n",
    "    theta_nn = forward_pass(t)\n",
    "    data_loss = lambda1 * MSE_func(theta_nn, theta_data)\n",
    "    \n",
    "    return  data_loss\n",
    "    \n",
    "# Initialize a list to store the loss values\n",
    "loss_values_ann = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = NeuralNetworkLoss(theta_ann,\n",
    "                             t_data,\n",
    "                             theta_data)    # must be (1. nn output, 2. target)\n",
    "    \n",
    "    # Append the current loss value to the list\n",
    "    loss_values_ann.append(loss.item())\n",
    "    \n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_pred_ann = theta_ann(t_test).to(device)\n",
    "\n",
    "print(f'Relative error: {relative_l2_error(theta_pred_ann, theta_test)}')\n",
    "\n",
    "plot_comparison(t_test, theta_num, theta_pred_ann, loss_values_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Physics-Informed Neural Network\n",
    "For this task we use the same noisy **training data** but in this case, we train the PINN to directly approximate the solution to the differential equation, i.e.,\n",
    "\n",
    "$$\n",
    "\\theta_{PINN}(t; \\Theta) \\approx \\theta(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 901\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neural network\n",
    "theta_pinn = NeuralNetwork(hidden_layers).to(device)\n",
    "nparams = sum(p.numel() for p in theta_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(theta_pinn.parameters(), lr=learning_rate, \n",
    "                       betas= (0.9,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-Informed Loss function\n",
    "To train the PINN, we recall the pendulum model and define function $f_{ode}(t;g,l)$, $g_{ic}(0)$ and $h_{bc}(0)$ for the ODE, the initial condition and the boundary condition. Also, we replace the analytical solution $\\theta(t)$ with the PINN output $\\theta_{pinn}(t; \\Theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{ode}(t;\\theta_{pinn}):=&\\frac{d^2\\theta_{PINN}(t; \\Theta)}{dt^2}+\\frac{g}{l}\\sin(\\theta_{pinn}(t; \\Theta)) = 0\\\\\n",
    "g_{ic}(0;\\theta_{pinn}):=&\\theta_{pinn}(0; \\Theta) = \\theta_0\\\\\n",
    "h_{bc}(0;\\theta_{pinn}):=&\\theta_{pinn}'(0; \\Theta) = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Once again we use the $MSE$ and define the physics-informed loss function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta):= &\\frac{\\lambda_1}{N}\\sum_i\\left(f_{ode}(t_i;\\theta_{pinn})-0\\right)^2 \\quad \\text{ODE loss}\\\\\n",
    "                   & + \\lambda_2 (g_{ic}(0;\\theta_{pinn})-\\theta_0)^2 \\quad \\text{IC loss}\\\\\n",
    "                   & + \\lambda_3 (h_{bc}(0;\\theta_{pinn})-0)^2 \\quad \\text{BC loss}\\\\\n",
    "                   & + \\frac{\\lambda_4}{N}\\sum_i (\\theta_{pinn}(t_i; \\Theta) - \\theta_{data}(t_i))^2 \\quad \\text{DATA loss}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\lambda_{1,2,3,4}\\in\\mathbb{R}^+$ are positive (weigth) numbers, and $N$ is the number of samples. \n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> when we do not include the loss function related to the data, we are employing a data-free scheme; when we include the data, we are employing a data-driven scheme.\n",
    "</div>\n",
    "\n",
    "The training is performed by minimizing the loss function $\\mathcal{L}(\\Theta)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> Autodifferentiation (torch.autograd) is a powerful tool for calculating the gradients of the PINN with respect to its input to evaluate the loss function; for more information, refer to the tutorial.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define t = 0 for boundary an initial conditions \n",
    "t0 = torch.tensor(0., device=device, requires_grad=True).view(-1,1)\n",
    "\n",
    "# HINT: use grad funtion (a wraper for torch.autograd) to calculate the \n",
    "# derivatives of the ANN\n",
    "def PINNLoss(forward_pass, t_phys, t_data, theta_data, \n",
    "             lambda1 = 1, lambda2 = 1, lambda3 = 1, lambda4 = 1):\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    theta_pinn1 = forward_pass(t_phys)\n",
    "    #TODO: calculate the first and second derivatives\n",
    "    \n",
    "    #TODO: calculate the ODE loss\n",
    "    \n",
    "    g_ic = forward_pass(t0)\n",
    "    IC_loss = lambda2 * MSE_func(g_ic, torch.ones_like(g_ic)*theta0)\n",
    "    \n",
    "    #TODO: calculate boundary condition\n",
    "    \n",
    "    theta_nn2 = forward_pass(t_data)\n",
    "    data_loss = lambda4 * MSE_func(theta_nn2, theta_data)\n",
    "    \n",
    "    return ODE_loss + IC_loss + BC_loss + data_loss\n",
    "    \n",
    "# Initialize a list to store the loss values\n",
    "loss_values_pinn = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = PINNLoss(theta_pinn, t_test, t_data, theta_data)\n",
    "    \n",
    "    # Append the current loss value to the list\n",
    "    loss_values_pinn.append(loss.item())\n",
    "    \n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_pred_pinn = theta_pinn(t_test)\n",
    "\n",
    "print(f'Relative error: {relative_l2_error(theta_pred_pinn, theta_test)}')\n",
    "\n",
    "plot_comparison(t_test, theta_num, theta_pred_pinn, loss_values_pinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(t_test, theta_num, theta_pred_ann, loss_values_ann)\n",
    "plot_comparison(t_test, theta_num, theta_pred_pinn, loss_values_pinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercises**:\n",
    "1. Try increasing and deacresing the `std_deviation` parameter in the [Preparing the training data](#Preparing-the-training-data) to change the `SNR`. Also, change the `resample` and the `ctime` variables to simulate other scenarios. Retrain the ANN and the PINN, and compare the results.\n",
    "2. Increase and decrease the `lambdas` parameters of the loss function for both the ANN and the PINN.\n",
    "3. Increase and reduce the learning rate of the optimizer, and the number of training iterations.\n",
    "4. Change the number of hidden layers, the number of neurons, the activation functions of the NN model. \n",
    "\n",
    "## **Questions**:\n",
    "1. **How should overfitting be addressed in PINNs if the goal is to learn the underlying operators of the physical system?**  \n",
    "   <details>\n",
    "   <summary>Answer</summary>\n",
    "   When training PINNs to learn operators, overfitting occurs if the network memorizes specific training points (e.g., boundary or initial conditions) rather than capturing the general behavior of the operator across the domain. To address this, model regularization is essential—penalizing weight norms or implementing derivative-based regularization can help the network generalize. Expanding the training set to cover critical areas, adding controlled noise to training data, and carefully adjusting network complexity can further reduce overfitting. Monitoring the residual error across the domain also helps ensure the network learns the operator’s global behavior rather than memorizing specific patterns.\n",
    "   </details>\n",
    "\n",
    "\n",
    "2. **What advantages does using MSE in the loss function offer, given that this approach may underestimate the solution by not incorporating integral or variational formulations?**  \n",
    "   <details>\n",
    "   <summary>Answer</summary>\n",
    "   Using MSE as the loss function provides a straightforward and computationally efficient way to measure pointwise differences between the model predictions and target values. However, MSE focuses only on individual points, which may limit its capacity to capture the global behavior of the solution, particularly in complex domains. By incorporating integral or variational formulations, the loss function can reflect the solution’s behavior over the entire domain, potentially improving accuracy and stability. Despite these limitations, MSE remains popular because it simplifies implementation and reduces computational costs, making it suitable for many practical applications.\n",
    "   </details>\n",
    "\n",
    "\n",
    "3. **In what ways are PINNs advantageous compared to traditional numerical methods, considering the time required for training?**  \n",
    "   <details>\n",
    "   <summary>Answer</summary>\n",
    "   PINNs offer several advantages over traditional numerical methods, despite their typically longer training times. One key advantage is their flexibility in handling complex, high-dimensional domains and irregular geometries without requiring structured grids. PINNs can also incorporate additional constraints or data seamlessly, such as experimental measurements or boundary conditions. Unlike many numerical methods, PINNs generalize well across different initial and boundary conditions once trained, which can make them more versatile for scenarios requiring repeated simulations or parameter tuning. This flexibility and adaptability make PINNs a powerful tool for certain types of physics-informed modeling tasks where traditional methods may be limited.\n",
    "   </details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
