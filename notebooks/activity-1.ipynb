{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks vs. Physics-Informed Neural Networks\n",
    "\n",
    "By David Ortiz and Rodrigo Salas, 2024\n",
    "\n",
    "Explore the foundational paper on PINNs [here](https://www.sciencedirect.com/science/article/pii/S0021999118307125).\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Artificial Neural Networks (ANNs) are powerful tools for solving complex, data-driven tasks. However, they often require large amounts of data and may lack interpretability when dealing with physical systems. Physics-Informed Neural Networks (PINNs) address this by embedding known physical laws directly into the training process, making them especially useful for tasks where governing equations are available but data is limited. In this activity, we’ll explore both approaches by applying them to model the **oscillating pendulum**, a classic nonlinear system.\n",
    "\n",
    "### Activity Overview\n",
    "\n",
    "In this activity, we will code an Artificial Neural Network (ANN) and a Physics-Informed Neural Network (PINN) to solve the nonlinear mathematical model of an **oscillating pendulum**. This approach will highlight the benefits of integrating physical laws into the network's loss function.\n",
    "\n",
    "### Activity Goals\n",
    "\n",
    "By the end of this activity, you will be able to:\n",
    "\n",
    " - understand the need for numerical solutions in complex models\n",
    " - recognize the advantages of Physics-Informed Neural Networks (PINNs) over traditional Artificial Neural Networks (ANNs)\n",
    " - train data-driven PINNs using PyTorch\n",
    " - apply PINNs to solve nonlinear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical description of the oscillating pendulum\n",
    "We want to solve the mathematical problem related to the **oscillating pendulum**[(wiki)](https://en.wikipedia.org/wiki/Pendulum_(mechanics)):\n",
    "\n",
    "\n",
    "| ![GIF](../data/figures/Oscillating_pendulum.gif) | <img src=\"../data/figures/Pendulum_gravity.svg\" alt=\"Diagrama del proyecto\" width=\"300\"/> |\n",
    "|-------------------------------------------|-------------------------------------------|\n",
    "| Pendulum velocity and acceleration vectors  | Force diagram |\n",
    "\n",
    "\n",
    "Assumptions:\n",
    "- the rod is rigid and massless [(Homework)](https://en.wikipedia.org/wiki/Elastic_pendulum#:~:text=In%20physics%20and%20mathematics%2C%20in,%2Ddimensional%20spring%2Dmass%20system.)\n",
    "- the weight is a point mass\n",
    "- two dimensions [(Homework)](https://www.instagram.com/reel/CffUr64PjCx/?igsh=MWlmM2FscG9oYnp6bw%3D%3D)\n",
    "- no air resistance [(Homework)](https://www.youtube.com/watch?v=erveOJD_qv4&ab_channel=Lettherebemath)\n",
    "- gravitational field is uniform and the support does not move\n",
    "\n",
    "We are interested in find the vertical angle $\\theta(t) \\in [0, 2\\pi)$ such that:\n",
    "\n",
    "$$\n",
    "\\frac{d^2\\theta}{dt^2}+\\frac{g}{l}\\sin\\theta=0,\\quad\\theta(0)=\\theta_0,\\quad\\theta'(0)=0,\\quad t\\in\\mathbb{R}, \n",
    "$$\n",
    "\n",
    "where $g\\approx9.81m/s^2$, $l$ is the length of the rod and $t$ the temporal variable.\n",
    "\n",
    "Review on differential equations:\n",
    "- Why is this a non-linear differential equation?\n",
    "- It is an ordinary differential equation (ODE) or a partial differential equation (PDE)?\n",
    "- Which is the order, Which is the degree?\n",
    "\n",
    "A usefull method is to convert the model to a coupled system of EDOs:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d\\theta}{dt} &= \\omega, \\quad \\text{angular velocity}\\\\\n",
    "\\frac{d\\omega}{dt} & = -\\frac{g}{l}\\sin\\theta, \\quad \\text{angular acceleration}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Workflow\n",
    "1. Calculate the numerical solution of the oscillating pendulum non-linear model\n",
    "2. Prepare the training data by adding noise, resample, and cutting the time to simulate a real escenario\n",
    "3. Define the ANN model using PyTorch arquitecture and train using the prepared data. Plot the solution\n",
    "4. Define the PINN model using PyTorch arquitecture and train using the prepared data. Plot the solution\n",
    "5. Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Initial setup\n",
    "\n",
    "We begin by importing some usefull packages, and defining some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Import Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "# Import the time module to time our training process\n",
    "import time\n",
    "# Ignore Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Actualización de los parámetros de Matplotlib\n",
    "gray = '#5c5c5c' #'#5c5c5c' '000'\n",
    "mlp.rcParams.update(\n",
    "    {\n",
    "        \"image.cmap\" : 'viridis', # plasma, inferno, magma, cividis\n",
    "        \"text.color\" : gray,\n",
    "        \"xtick.color\" :gray,\n",
    "        \"ytick.color\" :gray,\n",
    "        \"axes.labelcolor\" : gray,\n",
    "        \"axes.edgecolor\" :gray,\n",
    "        \"axes.spines.right\" : False,\n",
    "        \"axes.spines.top\" : False,\n",
    "        \"axes.formatter.use_mathtext\": True,\n",
    "        \"axes.unicode_minus\": False,\n",
    "        \n",
    "        'font.size' : 15,\n",
    "        'interactive': False,\n",
    "        \"font.family\": 'sans-serif',\n",
    "        \"legend.loc\" : 'best',\n",
    "        'text.usetex': False,\n",
    "        'mathtext.fontset': 'stix',\n",
    "    }\n",
    ")\n",
    "\n",
    "# Util function to calculate the signal-to-noise ratio\n",
    "def calculate_snr(signal, noise):    \n",
    "    # Ensure numpy arrays\n",
    "    signal, noise = np.array(signal), np.array(noise)\n",
    "    \n",
    "    # Calculate the power of the signal and the noise\n",
    "    signal_power = np.mean(signal**2)\n",
    "    noise_power = np.mean(noise**2)\n",
    "    \n",
    "    # Calculate the SNR in decibels (dB)\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "# Util function to calculate the relative l2 error\n",
    "def relative_l2_error(u_num, u_ref):\n",
    "    # Calculate the L2 norm of the difference\n",
    "    l2_diff = torch.norm(u_num - u_ref, p=2)\n",
    "    \n",
    "    # Calculate the L2 norm of the reference\n",
    "    l2_ref = torch.norm(u_ref, p=2)\n",
    "    \n",
    "    # Calculate L2 relative error\n",
    "    relative_l2 = l2_diff / l2_ref\n",
    "    return relative_l2\n",
    "\n",
    "# Util function to plot the solutions\n",
    "def plot_comparison(time, theta_true, theta_pred, loss):\n",
    "    \n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    t_np = time.detach().cpu().data.numpy()\n",
    "    theta_pred_np = theta_pred.detach().cpu().data.numpy()\n",
    "\n",
    "    # Create a figure with 2 subplots\n",
    "    _, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot the true and predicted values\n",
    "    axs[0].plot(t_np, theta_true, label = r'$\\theta(t)$ (numerical solution)')\n",
    "    axs[0].plot(t_np, theta_pred_np, label = r'$\\theta_{pred}(t)$ (predicted solution) ')\n",
    "    axs[0].set_title('Angular displacement Numerical Vs. Predicted')\n",
    "    axs[0].set_xlabel(r'Time $(s)$')\n",
    "    axs[0].set_ylabel('Amplitude')\n",
    "    axs[0].set_ylim(-1,1.3)\n",
    "    axs[0].legend(loc='best', frameon=False)\n",
    "\n",
    "\n",
    "    # Plot the difference between the predicted and true values\n",
    "    difference = np.abs(theta_true.reshape(-1,1) - theta_pred_np.reshape(-1,1))\n",
    "    axs[1].plot(t_np, difference)\n",
    "    axs[1].set_title('Absolute Difference')\n",
    "    axs[1].set_xlabel(r'Time $(s)$')\n",
    "    axs[1].set_ylabel(r'$|\\theta(t) - \\theta_{pred}(t)|$')\n",
    "    # Display the plot\n",
    "    plt.legend(loc='best', frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the loss values recorded during training\n",
    "    # Create a figure with 1 subplots\n",
    "    _, axs = plt.subplots(1, 1, figsize=(6, 3))\n",
    "    axs.plot(loss)\n",
    "    axs.set_xlabel('Iteration')\n",
    "    axs.set_ylabel('Loss')\n",
    "    axs.set_yscale('log')\n",
    "    axs.set_xscale('log')\n",
    "    axs.set_title('Training Progress')\n",
    "    axs.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Util function to calculate tensor gradients with autodiff\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\"Computes the partial derivative of an output with respect \n",
    "    to an input.\n",
    "    Args:\n",
    "        outputs: (N, 1) tensor\n",
    "        inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs, \n",
    "                        grad_outputs=torch.ones_like(outputs), \n",
    "                        create_graph=True,\n",
    "                        )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Numerical solution of the oscillating pendulum\n",
    "For the numerical solution we use the [Runge-Kutta of forth order](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods) from `scipy`. We begin by defining the parameters for this example, the pendulum model, and the domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 9.81  # gravity acceleration (m/s^2)\n",
    "L = 1.0   # Pendulum's rod length (m)\n",
    "theta0 = np.pi / 4  # Initial condition (Position in rads)\n",
    "omega0 = 0.0        # Initial angular speed (rad/s)\n",
    "sample_freq = 100   # sample rate 100Hz\n",
    "\n",
    "# Simulation time\n",
    "t_span = (0, 10)  # from 0 to 10 seconds\n",
    "t_eval = np.linspace(t_span[0], t_span[1], sample_freq*t_span[1])  # Points to be evaluated\n",
    "\n",
    "# We define the system of coupled ODEs\n",
    "def pendulum(t, y):\n",
    "    theta, omega = y\n",
    "    dtheta_dt = omega\n",
    "    domega_dt = -(g / L) * np.sin(theta)\n",
    "    return [dtheta_dt, domega_dt]\n",
    "\n",
    "# Initial conditions\n",
    "y0 = [theta0, omega0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we solve the problem numerically using `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# Solve the initial value problem using Runge-Kutta 4th order\n",
    "num_sol = solve_ivp(pendulum, t_span, y0, t_eval=t_eval, method='RK45')\n",
    "\n",
    "# We extract the solutions\n",
    "theta_num = num_sol.y[0]\n",
    "omega_num = num_sol.y[1]\n",
    "\n",
    "# We graph the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t_eval, theta_num, label=r'Angular Displacement $\\theta(t)[rad]$')\n",
    "plt.plot(t_eval, omega_num, label=r'Angular Velocity $\\omega(t)[rad/s]$')\n",
    "plt.xlabel(r'Time $[s]$')\n",
    "plt.ylim(-2.5,3.3)\n",
    "plt.legend(loc='best', frameon=False)\n",
    "plt.title('Nonlinear Pendulum Solution')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the training data\n",
    "\n",
    "In the following, we consider the numerical solution as the **training data** that comes from the measures of a sensor. We add gaussian noise, resample and cut the data to $2.5s$ to test the performance of the ANN, simulating a real scenario. Also, we calculate the signal-to-noise ratio $SNR = 10\\log_{10} \\left(\\frac{P_{signal}}{P_{noise}}\\right)$, where $P_{signal}$ and $P_{noise}$ are the power of the signal and the noise, respectively, to get the amount of distortion in the signal. Finally, we call the noisy training data $\\theta_{data}(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gaussian noise\n",
    "std_deviation = 0.05\n",
    "noise = np.random.normal(0,std_deviation,theta_num.shape[0])\n",
    "theta_noisy = theta_num + noise\n",
    "print(f'SNR: {calculate_snr(theta_noisy, noise):.4f} dB')\n",
    "\n",
    "# Resample and cut to 2.5s\n",
    "resample = 5          # resample \n",
    "cut_time = int(2.5*sample_freq)  # 2.5s times 100Hz\n",
    "\n",
    "theta_data = theta_noisy[:cut_time:resample]\n",
    "t_data = t_eval[:cut_time:resample]\n",
    "\n",
    "# We graph the observed data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t_eval, theta_num, label=r'Angular Displacement (model) $\\theta(t)$ ')\n",
    "plt.plot(t_data, theta_data, label=r'Training data (measures) $\\theta_{data}(t)$ ')\n",
    "plt.xlabel(r'Time $[s]$')\n",
    "plt.ylabel(r'Angular displacement $[rad]$')\n",
    "plt.ylim(-1,1.3)\n",
    "plt.legend(loc='best', frameon=False)\n",
    "plt.title('Training data')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Artificial Neural Network\n",
    "\n",
    "We train the artificial neural network to directly approximate the solution to the differential equation, i.e.,\n",
    "\n",
    "$$\n",
    "\\theta_{NN}(t; \\Theta) \\approx \\theta(t)\n",
    "$$\n",
    "\n",
    "where $\\Theta$ are the free (trainable) parameters of the ANN. Now, we use `PyTorch` and define the neural network and, for this task, we will use the ADAM optimizer. Also, we convert the temporal domain and the observations to `torch.tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# training parameters\n",
    "hidden_layers = [1, 20, 20, 20, 1]\n",
    "learning_rate = 0.001\n",
    "training_iter = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "MSE_func = nn.MSELoss()\n",
    "\n",
    "# Convert the NumPy arrays to PyTorch tensors and add an extra dimension\n",
    "# test time Numpy array to Pytorch tensor\n",
    "t_test = torch.tensor(t_eval, device=device, requires_grad=True).view(-1,1).float()\n",
    "# train time Numpy array to Pytorch tensor\n",
    "t_data = torch.tensor(t_data, device=device, requires_grad=True).view(-1,1).float()\n",
    "# Numerical theta to test Numpy array to pytorch tensor \n",
    "theta_test = torch.tensor(theta_num, device=device, requires_grad=True).view(-1,1).float()\n",
    "# Numerical theta to train Numpy array to pytorch tensor \n",
    "theta_data = torch.tensor(theta_data, device=device, requires_grad=True).view(-1,1).float()\n",
    "\n",
    "# Define a neural network class with user defined layers and neurons\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, hlayers):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(hlayers[:-2])):\n",
    "            layers.append(nn.Linear(hlayers[i], hlayers[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hlayers[-2], hlayers[-1]))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        \"\"\"Xavier Glorot parameter initialization of the Neural Network\n",
    "        \"\"\"\n",
    "        def init_normal(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight) # Xavier\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 901\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neural network \n",
    "theta_ann = NeuralNetwork(hidden_layers).to(device)\n",
    "nparams = sum(p.numel() for p in theta_ann.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(theta_ann.parameters(), lr=learning_rate, \n",
    "                       betas= (0.9,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "To train the ANN, it is mandatory to define the loss function. To this end, we consider the noisy data $\\theta_{data}(t)$ and use the mean squared error ($MSE$) over the colocation points (samples over the domain) $\\{t_i\\}_N$, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta) := \\lambda_1 MSE(\\theta_{NN}(t; \\Theta), \\theta_{data}(t)) = \\frac{\\lambda_1}{N}\\sum_i (\\theta_{NN}(t_i; \\Theta) - \\theta_{data}(t_i))^2\n",
    "$$\n",
    "\n",
    "where $\\lambda_1\\in\\mathbb{R}^+$ is a positive (weigth) number, and $N$ is the number of samples. The training is performed by minimizing the loss function $\\mathcal{L}(\\Theta)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
    "$$\n",
    "\n",
    "\n",
    "Now, we use `PyTorch` and define the neural network, the function loss and, for this task, we will use the ADAM optimizer. Also, we convert the temporal domain and the observations to `torch.tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetworkLoss(forward_pass, t, theta_data, lambda1 = 1):\n",
    "    \n",
    "    theta_nn = forward_pass(t)\n",
    "    data_loss = lambda1 * MSE_func(theta_nn, theta_data)\n",
    "    \n",
    "    return  data_loss\n",
    "    \n",
    "# Initialize a list to store the loss values\n",
    "loss_values_ann = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = NeuralNetworkLoss(theta_ann,\n",
    "                             t_data,\n",
    "                             theta_data)    # must be (1. nn output, 2. target)\n",
    "    \n",
    "    # Append the current loss value to the list\n",
    "    loss_values_ann.append(loss.item())\n",
    "    \n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_pred_ann = theta_ann(t_test).to(device)\n",
    "\n",
    "print(f'Relative error: {relative_l2_error(theta_pred_ann, theta_test)}')\n",
    "\n",
    "plot_comparison(t_test, theta_num, theta_pred_ann, loss_values_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Physics-Informed Neural Network\n",
    "For this task we use the same noisy **training data** but in this case, we train the PINN to directly approximate the solution to the differential equation, i.e.,\n",
    "\n",
    "$$\n",
    "\\theta_{PINN}(t; \\Theta) \\approx \\theta(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 901\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the neural network\n",
    "theta_pinn = NeuralNetwork(hidden_layers).to(device)\n",
    "nparams = sum(p.numel() for p in theta_pinn.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {nparams}')\n",
    "\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(theta_pinn.parameters(), lr=learning_rate, \n",
    "                       betas= (0.9,0.999), eps = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-Informed Loss function\n",
    "To train the PINN, we recall the pendulum model and define function $f_{ode}(t;g,l)$, $g_{ic}(0)$ and $h_{bc}(0)$ for the ODE, the initial condition and the boundary condition. Also, we replace the analytical solution $\\theta(t)$ with the PINN output $\\theta_{pinn}(t; \\Theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{ode}(t;\\theta_{pinn}):=&\\frac{d^2\\theta_{PINN}(t; \\Theta)}{dt^2}+\\frac{g}{l}\\sin(\\theta_{pinn}(t; \\Theta)) = 0\\\\\n",
    "g_{ic}(0;\\theta_{pinn}):=&\\theta_{pinn}(0; \\Theta) = \\theta_0\\\\\n",
    "h_{bc}(0;\\theta_{pinn}):=&\\theta_{pinn}'(0; \\Theta) = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Once again we use the $MSE$ and define the physics-informed loss function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta):= &\\frac{\\lambda_1}{N}\\sum_i\\left(f_{ode}(t_i;\\theta_{pinn})-0\\right)^2 \\quad \\text{ODE loss}\\\\\n",
    "                   & + \\lambda_2 (g_{ic}(0;\\theta_{pinn})-\\theta_0)^2 \\quad \\text{IC loss}\\\\\n",
    "                   & + \\lambda_3 (h_{bc}(0;\\theta_{pinn})-0)^2 \\quad \\text{BC loss}\\\\\n",
    "                   & + \\frac{\\lambda_4}{N}\\sum_i (\\theta_{pinn}(t_i; \\Theta) - \\theta_{data}(t_i))^2 \\quad \\text{DATA loss}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\lambda_{1,2,3,4}\\in\\mathbb{R}^+$ are positive (weigth) numbers, and $N$ is the number of samples. \n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> when we do not include the loss function related to the data, we are employing a data-free scheme; when we include the data, we are employing a data-driven scheme.\n",
    "</div>\n",
    "\n",
    "The training is performed by minimizing the loss function $\\mathcal{L}(\\Theta)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\min_{\\Theta\\in\\mathbb{R}} \\mathcal{L}(\\Theta)\\rightarrow 0\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-info\"\n",
    "    style=\"background-color:#5c5c5c;color:#000000;border-color:#000000\">\n",
    "  <strong>REMARK!</strong> Autodifferentiation (torch.autograd) is a powerful tool for calculating the gradients of the PINN with respect to its input to evaluate the loss function; for more information, refer to the tutorial.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define t = 0 for boundary an initial conditions \n",
    "t0 = torch.tensor(0., device=device, requires_grad=True).view(-1,1)\n",
    "\n",
    "# HINT: use grad funtion (a wraper for torch.autograd) to calculate the \n",
    "# derivatives of the ANN\n",
    "def PINNLoss(forward_pass, t_phys, t_data, theta_data, \n",
    "             lambda1 = 1, lambda2 = 1, lambda3 = 1, lambda4 = 1):\n",
    "\n",
    "    # ANN output, first and second derivatives\n",
    "    theta_pinn1 = forward_pass(t_phys)\n",
    "    #TODO: calculate the first and second derivatives\n",
    "    \n",
    "    #TODO: calculate the ODE loss\n",
    "    \n",
    "    g_ic = forward_pass(t0)\n",
    "    IC_loss = lambda2 * MSE_func(g_ic, torch.ones_like(g_ic)*theta0)\n",
    "    \n",
    "    #TODO: calculate boundary condition\n",
    "    \n",
    "    theta_nn2 = forward_pass(t_data)\n",
    "    data_loss = lambda4 * MSE_func(theta_nn2, theta_data)\n",
    "    \n",
    "    return ODE_loss + IC_loss + BC_loss + data_loss\n",
    "    \n",
    "# Initialize a list to store the loss values\n",
    "loss_values_pinn = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(training_iter):\n",
    "    \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "\n",
    "    # input x and predict based on x\n",
    "    loss = PINNLoss(theta_pinn, t_test, t_data, theta_data)\n",
    "    \n",
    "    # Append the current loss value to the list\n",
    "    loss_values_pinn.append(loss.item())\n",
    "    \n",
    "    if i % 1000 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    loss.backward() # compute gradients (backpropagation)\n",
    "    optimizer.step() # update the ANN weigths\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_pred_pinn = theta_pinn(t_test)\n",
    "\n",
    "print(f'Relative error: {relative_l2_error(theta_pred_pinn, theta_test)}')\n",
    "\n",
    "plot_comparison(t_test, theta_num, theta_pred_pinn, loss_values_pinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(t_test, theta_num, theta_pred_ann, loss_values_ann)\n",
    "plot_comparison(t_test, theta_num, theta_pred_pinn, loss_values_pinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercises**:\n",
    "1. Try increasing and deacresing the `std_deviation` parameter in the [Preparing the training data](#Preparing-the-training-data) to change the `SNR`. Also, change the `resample` and the `ctime` variables to simulate other scenarios. Retrain the ANN and the PINN, and compare the results.\n",
    "2. Increase and decrease the `lambdas` parameters of the loss function for both the ANN and the PINN.\n",
    "3. Increase and reduce the learning rate of the optimizer, and the number of training iterations.\n",
    "4. Change the number of hidden layers, the number of neurons, the activation functions of the NN model. \n",
    "\n",
    "## **Questions**:\n",
    "1. **How should overfitting be addressed in PINNs if the goal is to learn the underlying operators of the physical system?**  \n",
    "   <details>\n",
    "   <summary>Answer</summary>\n",
    "   When training PINNs to learn operators, overfitting occurs if the network memorizes specific training points (e.g., boundary or initial conditions) rather than capturing the general behavior of the operator across the domain. To address this, model regularization is essential—penalizing weight norms or implementing derivative-based regularization can help the network generalize. Expanding the training set to cover critical areas, adding controlled noise to training data, and carefully adjusting network complexity can further reduce overfitting. Monitoring the residual error across the domain also helps ensure the network learns the operator’s global behavior rather than memorizing specific patterns.\n",
    "   </details>\n",
    "\n",
    "\n",
    "2. **What advantages does using MSE in the loss function offer, given that this approach may underestimate the solution by not incorporating integral or variational formulations?**  \n",
    "   <details>\n",
    "   <summary>Answer</summary>\n",
    "   Using MSE as the loss function provides a straightforward and computationally efficient way to measure pointwise differences between the model predictions and target values. However, MSE focuses only on individual points, which may limit its capacity to capture the global behavior of the solution, particularly in complex domains. By incorporating integral or variational formulations, the loss function can reflect the solution’s behavior over the entire domain, potentially improving accuracy and stability. Despite these limitations, MSE remains popular because it simplifies implementation and reduces computational costs, making it suitable for many practical applications.\n",
    "   </details>\n",
    "\n",
    "\n",
    "3. **In what ways are PINNs advantageous compared to traditional numerical methods, considering the time required for training?**  \n",
    "   <details>\n",
    "   <summary>Answer</summary>\n",
    "   PINNs offer several advantages over traditional numerical methods, despite their typically longer training times. One key advantage is their flexibility in handling complex, high-dimensional domains and irregular geometries without requiring structured grids. PINNs can also incorporate additional constraints or data seamlessly, such as experimental measurements or boundary conditions. Unlike many numerical methods, PINNs generalize well across different initial and boundary conditions once trained, which can make them more versatile for scenarios requiring repeated simulations or parameter tuning. This flexibility and adaptability make PINNs a powerful tool for certain types of physics-informed modeling tasks where traditional methods may be limited.\n",
    "   </details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
